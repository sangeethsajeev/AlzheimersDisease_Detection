{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Safna's Project Notebook - LDA + Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hV9QQi7q1Ui"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnDg-oQAq_3U",
        "outputId": "b9960dce-dee9-4231-b679-35ecbd304f1d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EL-6s68rB3l"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Safna's Project/Merged_DF/Merged_DF.csv\",index_col='Unnamed: 0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "RM5Z11pUlMpd",
        "outputId": "a2ee6ec2-c085-400f-afa5-8ea07234ab14"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>ILMN_1653355</th>\n",
              "      <th>ILMN_1698554</th>\n",
              "      <th>ILMN_2061446</th>\n",
              "      <th>ILMN_1688755</th>\n",
              "      <th>ILMN_1653165</th>\n",
              "      <th>ILMN_1662364</th>\n",
              "      <th>ILMN_1674698</th>\n",
              "      <th>ILMN_1700461</th>\n",
              "      <th>ILMN_1784269</th>\n",
              "      <th>ILMN_2096191</th>\n",
              "      <th>ILMN_1703743</th>\n",
              "      <th>ILMN_1687609</th>\n",
              "      <th>ILMN_1766054</th>\n",
              "      <th>ILMN_1747627</th>\n",
              "      <th>ILMN_1749001</th>\n",
              "      <th>ILMN_1795507</th>\n",
              "      <th>ILMN_1743205</th>\n",
              "      <th>ILMN_1665730</th>\n",
              "      <th>ILMN_1687840</th>\n",
              "      <th>ILMN_2343048</th>\n",
              "      <th>ILMN_1802404</th>\n",
              "      <th>ILMN_1776119</th>\n",
              "      <th>ILMN_1732140</th>\n",
              "      <th>ILMN_1774344</th>\n",
              "      <th>ILMN_1677814</th>\n",
              "      <th>ILMN_2194009</th>\n",
              "      <th>ILMN_1706531</th>\n",
              "      <th>ILMN_1725675</th>\n",
              "      <th>ILMN_1772189</th>\n",
              "      <th>ILMN_2330267</th>\n",
              "      <th>ILMN_1676846</th>\n",
              "      <th>ILMN_2392635</th>\n",
              "      <th>ILMN_1763875</th>\n",
              "      <th>ILMN_2329927</th>\n",
              "      <th>ILMN_1770031</th>\n",
              "      <th>ILMN_1737475</th>\n",
              "      <th>ILMN_2318685</th>\n",
              "      <th>ILMN_2245305</th>\n",
              "      <th>ILMN_1707925</th>\n",
              "      <th>...</th>\n",
              "      <th>ILMN_1734254</th>\n",
              "      <th>ILMN_1738124</th>\n",
              "      <th>ILMN_1691702</th>\n",
              "      <th>ILMN_1715718</th>\n",
              "      <th>ILMN_1791388</th>\n",
              "      <th>ILMN_1789364</th>\n",
              "      <th>ILMN_1794122</th>\n",
              "      <th>ILMN_1688346</th>\n",
              "      <th>ILMN_1749809</th>\n",
              "      <th>ILMN_1728710</th>\n",
              "      <th>ILMN_2151075</th>\n",
              "      <th>ILMN_1727574</th>\n",
              "      <th>ILMN_2190414</th>\n",
              "      <th>ILMN_1802053</th>\n",
              "      <th>ILMN_1726578</th>\n",
              "      <th>ILMN_1745148</th>\n",
              "      <th>ILMN_1812478</th>\n",
              "      <th>ILMN_1750044</th>\n",
              "      <th>ILMN_1788738</th>\n",
              "      <th>ILMN_1747102</th>\n",
              "      <th>ILMN_1662383</th>\n",
              "      <th>ILMN_1703015</th>\n",
              "      <th>ILMN_1809566</th>\n",
              "      <th>ILMN_1726512</th>\n",
              "      <th>ILMN_1812856</th>\n",
              "      <th>ILMN_2150654</th>\n",
              "      <th>ILMN_1777061</th>\n",
              "      <th>ILMN_1796119</th>\n",
              "      <th>ILMN_1712556</th>\n",
              "      <th>ILMN_2374633</th>\n",
              "      <th>ILMN_1743643</th>\n",
              "      <th>ILMN_1656676</th>\n",
              "      <th>ILMN_2371169</th>\n",
              "      <th>ILMN_1701875</th>\n",
              "      <th>ILMN_1786396</th>\n",
              "      <th>ILMN_1653618</th>\n",
              "      <th>ILMN_2137536</th>\n",
              "      <th>status_encoded</th>\n",
              "      <th>ethinicity_encoded</th>\n",
              "      <th>gender_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65</td>\n",
              "      <td>7.650277</td>\n",
              "      <td>8.148446</td>\n",
              "      <td>8.226970</td>\n",
              "      <td>9.139284</td>\n",
              "      <td>8.237717</td>\n",
              "      <td>9.616626</td>\n",
              "      <td>8.031522</td>\n",
              "      <td>8.309996</td>\n",
              "      <td>7.729847</td>\n",
              "      <td>7.843823</td>\n",
              "      <td>9.662915</td>\n",
              "      <td>8.077568</td>\n",
              "      <td>9.428345</td>\n",
              "      <td>7.865845</td>\n",
              "      <td>7.815536</td>\n",
              "      <td>7.460108</td>\n",
              "      <td>9.002055</td>\n",
              "      <td>7.679191</td>\n",
              "      <td>7.978732</td>\n",
              "      <td>7.610226</td>\n",
              "      <td>7.658746</td>\n",
              "      <td>8.017586</td>\n",
              "      <td>7.467655</td>\n",
              "      <td>7.452172</td>\n",
              "      <td>9.175609</td>\n",
              "      <td>7.381022</td>\n",
              "      <td>9.105610</td>\n",
              "      <td>7.467136</td>\n",
              "      <td>8.087267</td>\n",
              "      <td>7.783330</td>\n",
              "      <td>8.044322</td>\n",
              "      <td>8.720449</td>\n",
              "      <td>9.795684</td>\n",
              "      <td>8.728792</td>\n",
              "      <td>7.803804</td>\n",
              "      <td>7.597712</td>\n",
              "      <td>7.774922</td>\n",
              "      <td>8.102663</td>\n",
              "      <td>7.568479</td>\n",
              "      <td>...</td>\n",
              "      <td>7.921712</td>\n",
              "      <td>7.470530</td>\n",
              "      <td>7.722785</td>\n",
              "      <td>7.863020</td>\n",
              "      <td>9.762821</td>\n",
              "      <td>7.740889</td>\n",
              "      <td>7.486492</td>\n",
              "      <td>7.993250</td>\n",
              "      <td>7.470026</td>\n",
              "      <td>7.638297</td>\n",
              "      <td>7.894740</td>\n",
              "      <td>8.556778</td>\n",
              "      <td>7.638511</td>\n",
              "      <td>8.038134</td>\n",
              "      <td>7.492668</td>\n",
              "      <td>10.104193</td>\n",
              "      <td>8.347094</td>\n",
              "      <td>7.641566</td>\n",
              "      <td>7.616271</td>\n",
              "      <td>7.522899</td>\n",
              "      <td>7.571046</td>\n",
              "      <td>8.670722</td>\n",
              "      <td>8.101310</td>\n",
              "      <td>8.178689</td>\n",
              "      <td>8.788201</td>\n",
              "      <td>8.646717</td>\n",
              "      <td>8.393199</td>\n",
              "      <td>7.646406</td>\n",
              "      <td>8.183159</td>\n",
              "      <td>7.561927</td>\n",
              "      <td>8.245221</td>\n",
              "      <td>10.618034</td>\n",
              "      <td>11.500615</td>\n",
              "      <td>11.972311</td>\n",
              "      <td>8.561519</td>\n",
              "      <td>7.609590</td>\n",
              "      <td>7.441231</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66</td>\n",
              "      <td>7.905123</td>\n",
              "      <td>8.315475</td>\n",
              "      <td>8.030268</td>\n",
              "      <td>8.549185</td>\n",
              "      <td>8.169469</td>\n",
              "      <td>9.835654</td>\n",
              "      <td>7.783841</td>\n",
              "      <td>8.079420</td>\n",
              "      <td>7.833218</td>\n",
              "      <td>7.603615</td>\n",
              "      <td>10.043341</td>\n",
              "      <td>8.112037</td>\n",
              "      <td>10.111046</td>\n",
              "      <td>8.298427</td>\n",
              "      <td>7.436811</td>\n",
              "      <td>7.681562</td>\n",
              "      <td>10.325897</td>\n",
              "      <td>7.583687</td>\n",
              "      <td>7.956272</td>\n",
              "      <td>8.060253</td>\n",
              "      <td>7.665396</td>\n",
              "      <td>7.849625</td>\n",
              "      <td>7.492799</td>\n",
              "      <td>7.555516</td>\n",
              "      <td>8.916246</td>\n",
              "      <td>7.347596</td>\n",
              "      <td>9.084694</td>\n",
              "      <td>7.413566</td>\n",
              "      <td>8.866248</td>\n",
              "      <td>7.688971</td>\n",
              "      <td>7.753245</td>\n",
              "      <td>9.138855</td>\n",
              "      <td>9.917787</td>\n",
              "      <td>8.674218</td>\n",
              "      <td>7.843177</td>\n",
              "      <td>7.625132</td>\n",
              "      <td>7.718231</td>\n",
              "      <td>8.419514</td>\n",
              "      <td>7.454720</td>\n",
              "      <td>...</td>\n",
              "      <td>8.021802</td>\n",
              "      <td>7.501189</td>\n",
              "      <td>7.533398</td>\n",
              "      <td>7.804090</td>\n",
              "      <td>9.656795</td>\n",
              "      <td>7.571105</td>\n",
              "      <td>7.691394</td>\n",
              "      <td>7.993739</td>\n",
              "      <td>7.459030</td>\n",
              "      <td>7.708772</td>\n",
              "      <td>7.917870</td>\n",
              "      <td>8.533683</td>\n",
              "      <td>7.699531</td>\n",
              "      <td>8.161383</td>\n",
              "      <td>7.501998</td>\n",
              "      <td>10.998025</td>\n",
              "      <td>8.980737</td>\n",
              "      <td>7.491495</td>\n",
              "      <td>7.608536</td>\n",
              "      <td>7.525090</td>\n",
              "      <td>7.886355</td>\n",
              "      <td>8.299426</td>\n",
              "      <td>8.054065</td>\n",
              "      <td>8.252783</td>\n",
              "      <td>8.788201</td>\n",
              "      <td>9.546520</td>\n",
              "      <td>8.448941</td>\n",
              "      <td>7.618809</td>\n",
              "      <td>7.999389</td>\n",
              "      <td>7.720191</td>\n",
              "      <td>8.424180</td>\n",
              "      <td>11.289126</td>\n",
              "      <td>11.925122</td>\n",
              "      <td>11.936102</td>\n",
              "      <td>8.697649</td>\n",
              "      <td>7.740773</td>\n",
              "      <td>7.591333</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>67</td>\n",
              "      <td>7.610028</td>\n",
              "      <td>7.912023</td>\n",
              "      <td>7.847137</td>\n",
              "      <td>9.734130</td>\n",
              "      <td>8.173694</td>\n",
              "      <td>9.680070</td>\n",
              "      <td>8.103542</td>\n",
              "      <td>8.282269</td>\n",
              "      <td>7.673322</td>\n",
              "      <td>7.888384</td>\n",
              "      <td>9.822380</td>\n",
              "      <td>7.850279</td>\n",
              "      <td>8.550187</td>\n",
              "      <td>7.832568</td>\n",
              "      <td>7.607402</td>\n",
              "      <td>7.553998</td>\n",
              "      <td>8.904953</td>\n",
              "      <td>7.699037</td>\n",
              "      <td>7.884115</td>\n",
              "      <td>7.766951</td>\n",
              "      <td>7.733130</td>\n",
              "      <td>7.912797</td>\n",
              "      <td>7.573484</td>\n",
              "      <td>7.460622</td>\n",
              "      <td>8.256183</td>\n",
              "      <td>7.584703</td>\n",
              "      <td>8.971329</td>\n",
              "      <td>7.402659</td>\n",
              "      <td>8.098414</td>\n",
              "      <td>7.982504</td>\n",
              "      <td>7.857816</td>\n",
              "      <td>9.216399</td>\n",
              "      <td>9.937217</td>\n",
              "      <td>8.001208</td>\n",
              "      <td>7.747969</td>\n",
              "      <td>7.626327</td>\n",
              "      <td>7.915751</td>\n",
              "      <td>8.132416</td>\n",
              "      <td>7.462959</td>\n",
              "      <td>...</td>\n",
              "      <td>7.838845</td>\n",
              "      <td>7.583151</td>\n",
              "      <td>7.669813</td>\n",
              "      <td>7.672358</td>\n",
              "      <td>9.358193</td>\n",
              "      <td>7.831403</td>\n",
              "      <td>7.452582</td>\n",
              "      <td>7.887924</td>\n",
              "      <td>7.496336</td>\n",
              "      <td>7.804778</td>\n",
              "      <td>7.812287</td>\n",
              "      <td>8.885373</td>\n",
              "      <td>7.773634</td>\n",
              "      <td>8.538723</td>\n",
              "      <td>7.599220</td>\n",
              "      <td>10.435788</td>\n",
              "      <td>8.604181</td>\n",
              "      <td>7.518454</td>\n",
              "      <td>7.549281</td>\n",
              "      <td>7.439370</td>\n",
              "      <td>7.871195</td>\n",
              "      <td>8.619465</td>\n",
              "      <td>8.169948</td>\n",
              "      <td>8.169469</td>\n",
              "      <td>8.619189</td>\n",
              "      <td>9.008516</td>\n",
              "      <td>8.163792</td>\n",
              "      <td>7.581486</td>\n",
              "      <td>8.206669</td>\n",
              "      <td>7.787021</td>\n",
              "      <td>8.278769</td>\n",
              "      <td>11.068854</td>\n",
              "      <td>11.538169</td>\n",
              "      <td>11.714840</td>\n",
              "      <td>8.889266</td>\n",
              "      <td>7.830947</td>\n",
              "      <td>7.711558</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>67</td>\n",
              "      <td>7.647674</td>\n",
              "      <td>8.064728</td>\n",
              "      <td>8.172226</td>\n",
              "      <td>9.832317</td>\n",
              "      <td>8.086709</td>\n",
              "      <td>9.965456</td>\n",
              "      <td>8.131037</td>\n",
              "      <td>8.163628</td>\n",
              "      <td>7.691114</td>\n",
              "      <td>7.644093</td>\n",
              "      <td>10.226752</td>\n",
              "      <td>7.907394</td>\n",
              "      <td>8.584056</td>\n",
              "      <td>7.994788</td>\n",
              "      <td>7.606016</td>\n",
              "      <td>7.735995</td>\n",
              "      <td>9.508634</td>\n",
              "      <td>7.595166</td>\n",
              "      <td>7.643311</td>\n",
              "      <td>7.608362</td>\n",
              "      <td>7.665561</td>\n",
              "      <td>8.000468</td>\n",
              "      <td>7.410198</td>\n",
              "      <td>7.636140</td>\n",
              "      <td>9.738726</td>\n",
              "      <td>7.439620</td>\n",
              "      <td>9.003180</td>\n",
              "      <td>7.467165</td>\n",
              "      <td>8.312623</td>\n",
              "      <td>7.726915</td>\n",
              "      <td>7.442829</td>\n",
              "      <td>9.283740</td>\n",
              "      <td>9.940023</td>\n",
              "      <td>8.489772</td>\n",
              "      <td>7.675784</td>\n",
              "      <td>7.854397</td>\n",
              "      <td>7.909566</td>\n",
              "      <td>8.396654</td>\n",
              "      <td>7.429785</td>\n",
              "      <td>...</td>\n",
              "      <td>7.606291</td>\n",
              "      <td>7.578710</td>\n",
              "      <td>7.803660</td>\n",
              "      <td>7.744011</td>\n",
              "      <td>10.091509</td>\n",
              "      <td>7.584471</td>\n",
              "      <td>7.545805</td>\n",
              "      <td>7.825747</td>\n",
              "      <td>7.435316</td>\n",
              "      <td>7.615556</td>\n",
              "      <td>7.807305</td>\n",
              "      <td>9.041828</td>\n",
              "      <td>7.524073</td>\n",
              "      <td>8.287234</td>\n",
              "      <td>7.374598</td>\n",
              "      <td>10.639769</td>\n",
              "      <td>8.487496</td>\n",
              "      <td>7.589695</td>\n",
              "      <td>7.663742</td>\n",
              "      <td>7.532033</td>\n",
              "      <td>7.909078</td>\n",
              "      <td>8.254265</td>\n",
              "      <td>7.923285</td>\n",
              "      <td>8.242266</td>\n",
              "      <td>8.984602</td>\n",
              "      <td>8.893517</td>\n",
              "      <td>7.935821</td>\n",
              "      <td>7.605167</td>\n",
              "      <td>8.217947</td>\n",
              "      <td>7.560732</td>\n",
              "      <td>8.230026</td>\n",
              "      <td>10.527572</td>\n",
              "      <td>11.685271</td>\n",
              "      <td>12.224006</td>\n",
              "      <td>8.665815</td>\n",
              "      <td>7.744111</td>\n",
              "      <td>7.649596</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>67</td>\n",
              "      <td>7.676041</td>\n",
              "      <td>7.977451</td>\n",
              "      <td>8.144916</td>\n",
              "      <td>9.557300</td>\n",
              "      <td>8.384872</td>\n",
              "      <td>9.763432</td>\n",
              "      <td>8.109347</td>\n",
              "      <td>8.181645</td>\n",
              "      <td>7.581818</td>\n",
              "      <td>7.963825</td>\n",
              "      <td>9.948150</td>\n",
              "      <td>8.220269</td>\n",
              "      <td>9.444721</td>\n",
              "      <td>7.765828</td>\n",
              "      <td>7.538710</td>\n",
              "      <td>7.594694</td>\n",
              "      <td>9.667120</td>\n",
              "      <td>7.611544</td>\n",
              "      <td>7.823204</td>\n",
              "      <td>7.719722</td>\n",
              "      <td>7.766422</td>\n",
              "      <td>8.014104</td>\n",
              "      <td>7.490641</td>\n",
              "      <td>7.427067</td>\n",
              "      <td>8.855090</td>\n",
              "      <td>7.474749</td>\n",
              "      <td>9.359667</td>\n",
              "      <td>7.448467</td>\n",
              "      <td>8.131969</td>\n",
              "      <td>7.678227</td>\n",
              "      <td>7.676978</td>\n",
              "      <td>9.245827</td>\n",
              "      <td>9.933470</td>\n",
              "      <td>8.927955</td>\n",
              "      <td>7.588295</td>\n",
              "      <td>7.601251</td>\n",
              "      <td>8.064056</td>\n",
              "      <td>7.943285</td>\n",
              "      <td>7.497216</td>\n",
              "      <td>...</td>\n",
              "      <td>7.852698</td>\n",
              "      <td>7.542326</td>\n",
              "      <td>7.744761</td>\n",
              "      <td>7.661375</td>\n",
              "      <td>9.741335</td>\n",
              "      <td>7.808566</td>\n",
              "      <td>7.615489</td>\n",
              "      <td>7.918249</td>\n",
              "      <td>7.451993</td>\n",
              "      <td>7.493443</td>\n",
              "      <td>7.575234</td>\n",
              "      <td>8.947312</td>\n",
              "      <td>7.464135</td>\n",
              "      <td>8.111891</td>\n",
              "      <td>7.620563</td>\n",
              "      <td>10.513498</td>\n",
              "      <td>8.466743</td>\n",
              "      <td>7.520534</td>\n",
              "      <td>7.764400</td>\n",
              "      <td>7.431883</td>\n",
              "      <td>7.733685</td>\n",
              "      <td>8.199676</td>\n",
              "      <td>8.067626</td>\n",
              "      <td>8.015104</td>\n",
              "      <td>8.934105</td>\n",
              "      <td>8.967204</td>\n",
              "      <td>8.187785</td>\n",
              "      <td>7.449231</td>\n",
              "      <td>8.056766</td>\n",
              "      <td>7.531862</td>\n",
              "      <td>8.215804</td>\n",
              "      <td>10.833655</td>\n",
              "      <td>11.800960</td>\n",
              "      <td>11.989076</td>\n",
              "      <td>8.785981</td>\n",
              "      <td>7.713783</td>\n",
              "      <td>7.580389</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>83</td>\n",
              "      <td>6.025457</td>\n",
              "      <td>6.191944</td>\n",
              "      <td>7.191870</td>\n",
              "      <td>7.947948</td>\n",
              "      <td>6.755539</td>\n",
              "      <td>8.092437</td>\n",
              "      <td>6.468042</td>\n",
              "      <td>6.403759</td>\n",
              "      <td>6.241325</td>\n",
              "      <td>6.564325</td>\n",
              "      <td>8.000307</td>\n",
              "      <td>6.397057</td>\n",
              "      <td>7.222012</td>\n",
              "      <td>6.504706</td>\n",
              "      <td>6.122697</td>\n",
              "      <td>6.071590</td>\n",
              "      <td>7.646480</td>\n",
              "      <td>6.294688</td>\n",
              "      <td>6.649880</td>\n",
              "      <td>6.198136</td>\n",
              "      <td>6.270404</td>\n",
              "      <td>6.313891</td>\n",
              "      <td>6.099317</td>\n",
              "      <td>6.019087</td>\n",
              "      <td>7.656029</td>\n",
              "      <td>6.215946</td>\n",
              "      <td>7.428026</td>\n",
              "      <td>6.109199</td>\n",
              "      <td>6.852781</td>\n",
              "      <td>6.342467</td>\n",
              "      <td>6.391422</td>\n",
              "      <td>7.225209</td>\n",
              "      <td>8.021324</td>\n",
              "      <td>6.914030</td>\n",
              "      <td>6.080622</td>\n",
              "      <td>6.092792</td>\n",
              "      <td>6.322186</td>\n",
              "      <td>6.436023</td>\n",
              "      <td>5.978622</td>\n",
              "      <td>...</td>\n",
              "      <td>6.299097</td>\n",
              "      <td>6.185424</td>\n",
              "      <td>6.168091</td>\n",
              "      <td>6.403127</td>\n",
              "      <td>7.553589</td>\n",
              "      <td>6.286259</td>\n",
              "      <td>6.145382</td>\n",
              "      <td>6.343409</td>\n",
              "      <td>6.166388</td>\n",
              "      <td>6.154444</td>\n",
              "      <td>6.276808</td>\n",
              "      <td>6.592706</td>\n",
              "      <td>6.430796</td>\n",
              "      <td>7.278737</td>\n",
              "      <td>6.062116</td>\n",
              "      <td>8.739865</td>\n",
              "      <td>6.540948</td>\n",
              "      <td>6.395407</td>\n",
              "      <td>6.063348</td>\n",
              "      <td>6.203769</td>\n",
              "      <td>6.542782</td>\n",
              "      <td>7.240344</td>\n",
              "      <td>6.598752</td>\n",
              "      <td>6.381795</td>\n",
              "      <td>7.397203</td>\n",
              "      <td>7.234947</td>\n",
              "      <td>7.084310</td>\n",
              "      <td>6.210119</td>\n",
              "      <td>6.669304</td>\n",
              "      <td>6.181358</td>\n",
              "      <td>6.866880</td>\n",
              "      <td>8.659101</td>\n",
              "      <td>9.703754</td>\n",
              "      <td>10.998052</td>\n",
              "      <td>7.713113</td>\n",
              "      <td>6.203181</td>\n",
              "      <td>6.340159</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>713</th>\n",
              "      <td>83</td>\n",
              "      <td>6.091774</td>\n",
              "      <td>6.123798</td>\n",
              "      <td>7.382681</td>\n",
              "      <td>8.035608</td>\n",
              "      <td>6.920995</td>\n",
              "      <td>7.913669</td>\n",
              "      <td>6.370507</td>\n",
              "      <td>6.303158</td>\n",
              "      <td>6.339691</td>\n",
              "      <td>6.604721</td>\n",
              "      <td>7.816700</td>\n",
              "      <td>6.435386</td>\n",
              "      <td>7.109822</td>\n",
              "      <td>6.577170</td>\n",
              "      <td>6.024241</td>\n",
              "      <td>5.992126</td>\n",
              "      <td>6.830460</td>\n",
              "      <td>6.406476</td>\n",
              "      <td>6.645352</td>\n",
              "      <td>6.244714</td>\n",
              "      <td>6.112203</td>\n",
              "      <td>6.052734</td>\n",
              "      <td>6.148598</td>\n",
              "      <td>6.047861</td>\n",
              "      <td>6.934621</td>\n",
              "      <td>6.226893</td>\n",
              "      <td>6.879802</td>\n",
              "      <td>6.045579</td>\n",
              "      <td>6.437800</td>\n",
              "      <td>6.399150</td>\n",
              "      <td>6.473523</td>\n",
              "      <td>7.221883</td>\n",
              "      <td>7.990077</td>\n",
              "      <td>6.247180</td>\n",
              "      <td>6.379838</td>\n",
              "      <td>6.067529</td>\n",
              "      <td>6.329688</td>\n",
              "      <td>6.396954</td>\n",
              "      <td>5.976998</td>\n",
              "      <td>...</td>\n",
              "      <td>6.369548</td>\n",
              "      <td>6.041083</td>\n",
              "      <td>6.097158</td>\n",
              "      <td>6.297107</td>\n",
              "      <td>7.268392</td>\n",
              "      <td>6.334440</td>\n",
              "      <td>6.220699</td>\n",
              "      <td>6.597344</td>\n",
              "      <td>6.279882</td>\n",
              "      <td>6.327253</td>\n",
              "      <td>6.208397</td>\n",
              "      <td>6.866831</td>\n",
              "      <td>6.488981</td>\n",
              "      <td>7.308424</td>\n",
              "      <td>6.000767</td>\n",
              "      <td>8.481715</td>\n",
              "      <td>7.263523</td>\n",
              "      <td>6.360071</td>\n",
              "      <td>6.282923</td>\n",
              "      <td>6.328546</td>\n",
              "      <td>6.568216</td>\n",
              "      <td>7.406904</td>\n",
              "      <td>6.615226</td>\n",
              "      <td>6.524513</td>\n",
              "      <td>7.193188</td>\n",
              "      <td>6.820713</td>\n",
              "      <td>6.883545</td>\n",
              "      <td>6.298697</td>\n",
              "      <td>6.722318</td>\n",
              "      <td>6.164023</td>\n",
              "      <td>6.411104</td>\n",
              "      <td>8.149314</td>\n",
              "      <td>9.230673</td>\n",
              "      <td>10.294702</td>\n",
              "      <td>7.363716</td>\n",
              "      <td>6.335564</td>\n",
              "      <td>6.372464</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>82</td>\n",
              "      <td>6.088321</td>\n",
              "      <td>6.255484</td>\n",
              "      <td>7.118513</td>\n",
              "      <td>7.578105</td>\n",
              "      <td>6.752410</td>\n",
              "      <td>7.624695</td>\n",
              "      <td>6.458760</td>\n",
              "      <td>6.456058</td>\n",
              "      <td>6.267209</td>\n",
              "      <td>6.557088</td>\n",
              "      <td>7.637326</td>\n",
              "      <td>6.536048</td>\n",
              "      <td>8.327500</td>\n",
              "      <td>6.531331</td>\n",
              "      <td>6.160537</td>\n",
              "      <td>6.036437</td>\n",
              "      <td>7.621533</td>\n",
              "      <td>6.345700</td>\n",
              "      <td>6.503192</td>\n",
              "      <td>6.143915</td>\n",
              "      <td>6.142310</td>\n",
              "      <td>6.143007</td>\n",
              "      <td>6.059892</td>\n",
              "      <td>6.139461</td>\n",
              "      <td>7.113539</td>\n",
              "      <td>6.245877</td>\n",
              "      <td>7.321108</td>\n",
              "      <td>6.072792</td>\n",
              "      <td>6.365518</td>\n",
              "      <td>6.442756</td>\n",
              "      <td>6.380369</td>\n",
              "      <td>6.956418</td>\n",
              "      <td>8.054158</td>\n",
              "      <td>6.464440</td>\n",
              "      <td>6.478612</td>\n",
              "      <td>6.036811</td>\n",
              "      <td>6.118582</td>\n",
              "      <td>6.387609</td>\n",
              "      <td>6.238373</td>\n",
              "      <td>...</td>\n",
              "      <td>6.412300</td>\n",
              "      <td>6.181855</td>\n",
              "      <td>6.101060</td>\n",
              "      <td>6.384941</td>\n",
              "      <td>7.559047</td>\n",
              "      <td>6.215338</td>\n",
              "      <td>6.141551</td>\n",
              "      <td>6.718023</td>\n",
              "      <td>6.215411</td>\n",
              "      <td>6.302167</td>\n",
              "      <td>6.268427</td>\n",
              "      <td>6.305436</td>\n",
              "      <td>6.433343</td>\n",
              "      <td>7.260361</td>\n",
              "      <td>6.041584</td>\n",
              "      <td>8.409387</td>\n",
              "      <td>7.428963</td>\n",
              "      <td>6.294662</td>\n",
              "      <td>6.082149</td>\n",
              "      <td>6.224265</td>\n",
              "      <td>6.725640</td>\n",
              "      <td>7.288356</td>\n",
              "      <td>6.762998</td>\n",
              "      <td>6.662717</td>\n",
              "      <td>7.280857</td>\n",
              "      <td>6.956466</td>\n",
              "      <td>6.974612</td>\n",
              "      <td>6.210041</td>\n",
              "      <td>6.671604</td>\n",
              "      <td>6.235154</td>\n",
              "      <td>6.942359</td>\n",
              "      <td>8.218770</td>\n",
              "      <td>8.981358</td>\n",
              "      <td>10.267423</td>\n",
              "      <td>7.468304</td>\n",
              "      <td>6.333388</td>\n",
              "      <td>6.358689</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>88</td>\n",
              "      <td>6.005137</td>\n",
              "      <td>6.162781</td>\n",
              "      <td>6.884646</td>\n",
              "      <td>7.810472</td>\n",
              "      <td>6.618831</td>\n",
              "      <td>7.967337</td>\n",
              "      <td>6.605923</td>\n",
              "      <td>6.673488</td>\n",
              "      <td>6.243907</td>\n",
              "      <td>6.541603</td>\n",
              "      <td>7.685420</td>\n",
              "      <td>6.371822</td>\n",
              "      <td>7.782668</td>\n",
              "      <td>6.406314</td>\n",
              "      <td>6.114012</td>\n",
              "      <td>6.111761</td>\n",
              "      <td>7.737141</td>\n",
              "      <td>6.319466</td>\n",
              "      <td>6.498007</td>\n",
              "      <td>6.309278</td>\n",
              "      <td>6.211832</td>\n",
              "      <td>6.305510</td>\n",
              "      <td>6.085181</td>\n",
              "      <td>6.098994</td>\n",
              "      <td>7.256250</td>\n",
              "      <td>6.211291</td>\n",
              "      <td>7.241650</td>\n",
              "      <td>6.032668</td>\n",
              "      <td>6.398309</td>\n",
              "      <td>6.423645</td>\n",
              "      <td>6.368639</td>\n",
              "      <td>7.150617</td>\n",
              "      <td>8.054151</td>\n",
              "      <td>6.705869</td>\n",
              "      <td>6.371576</td>\n",
              "      <td>6.079311</td>\n",
              "      <td>6.106477</td>\n",
              "      <td>6.311087</td>\n",
              "      <td>6.273998</td>\n",
              "      <td>...</td>\n",
              "      <td>6.392218</td>\n",
              "      <td>6.091210</td>\n",
              "      <td>6.186592</td>\n",
              "      <td>6.487828</td>\n",
              "      <td>7.684302</td>\n",
              "      <td>6.291061</td>\n",
              "      <td>6.152289</td>\n",
              "      <td>6.624625</td>\n",
              "      <td>6.120987</td>\n",
              "      <td>6.316855</td>\n",
              "      <td>6.267967</td>\n",
              "      <td>6.582374</td>\n",
              "      <td>6.449674</td>\n",
              "      <td>7.469064</td>\n",
              "      <td>6.025309</td>\n",
              "      <td>8.350234</td>\n",
              "      <td>7.431486</td>\n",
              "      <td>6.192741</td>\n",
              "      <td>6.059229</td>\n",
              "      <td>6.114496</td>\n",
              "      <td>6.785388</td>\n",
              "      <td>7.330857</td>\n",
              "      <td>6.756816</td>\n",
              "      <td>6.544636</td>\n",
              "      <td>7.283032</td>\n",
              "      <td>6.987541</td>\n",
              "      <td>6.908084</td>\n",
              "      <td>6.247840</td>\n",
              "      <td>6.701660</td>\n",
              "      <td>6.223137</td>\n",
              "      <td>6.752188</td>\n",
              "      <td>8.487030</td>\n",
              "      <td>9.137028</td>\n",
              "      <td>10.492294</td>\n",
              "      <td>7.599343</td>\n",
              "      <td>6.431762</td>\n",
              "      <td>6.391373</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>86</td>\n",
              "      <td>6.116739</td>\n",
              "      <td>6.151224</td>\n",
              "      <td>7.056757</td>\n",
              "      <td>8.032515</td>\n",
              "      <td>6.668038</td>\n",
              "      <td>7.993801</td>\n",
              "      <td>6.569850</td>\n",
              "      <td>6.614604</td>\n",
              "      <td>6.372780</td>\n",
              "      <td>6.739602</td>\n",
              "      <td>7.513964</td>\n",
              "      <td>6.253959</td>\n",
              "      <td>8.389697</td>\n",
              "      <td>6.438828</td>\n",
              "      <td>6.125999</td>\n",
              "      <td>6.038305</td>\n",
              "      <td>7.549245</td>\n",
              "      <td>6.366505</td>\n",
              "      <td>6.458320</td>\n",
              "      <td>6.198815</td>\n",
              "      <td>6.184645</td>\n",
              "      <td>6.269684</td>\n",
              "      <td>6.053481</td>\n",
              "      <td>6.011146</td>\n",
              "      <td>6.805501</td>\n",
              "      <td>6.205474</td>\n",
              "      <td>7.155149</td>\n",
              "      <td>5.977905</td>\n",
              "      <td>6.176843</td>\n",
              "      <td>6.443089</td>\n",
              "      <td>6.519355</td>\n",
              "      <td>6.919424</td>\n",
              "      <td>8.123694</td>\n",
              "      <td>6.541376</td>\n",
              "      <td>6.571764</td>\n",
              "      <td>6.088611</td>\n",
              "      <td>6.078007</td>\n",
              "      <td>6.344500</td>\n",
              "      <td>6.204748</td>\n",
              "      <td>...</td>\n",
              "      <td>6.637192</td>\n",
              "      <td>6.030174</td>\n",
              "      <td>6.185430</td>\n",
              "      <td>6.287608</td>\n",
              "      <td>7.145479</td>\n",
              "      <td>6.333991</td>\n",
              "      <td>6.020312</td>\n",
              "      <td>6.937033</td>\n",
              "      <td>6.176610</td>\n",
              "      <td>6.525996</td>\n",
              "      <td>6.068787</td>\n",
              "      <td>6.755403</td>\n",
              "      <td>6.625737</td>\n",
              "      <td>7.120677</td>\n",
              "      <td>6.080683</td>\n",
              "      <td>7.993927</td>\n",
              "      <td>7.772551</td>\n",
              "      <td>6.334878</td>\n",
              "      <td>6.056322</td>\n",
              "      <td>6.119992</td>\n",
              "      <td>7.072445</td>\n",
              "      <td>7.443929</td>\n",
              "      <td>6.540162</td>\n",
              "      <td>6.689672</td>\n",
              "      <td>7.123181</td>\n",
              "      <td>6.885056</td>\n",
              "      <td>6.759775</td>\n",
              "      <td>6.201931</td>\n",
              "      <td>6.800933</td>\n",
              "      <td>6.367431</td>\n",
              "      <td>6.847691</td>\n",
              "      <td>8.143475</td>\n",
              "      <td>8.896626</td>\n",
              "      <td>9.970382</td>\n",
              "      <td>7.348540</td>\n",
              "      <td>6.425247</td>\n",
              "      <td>6.467218</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>717 rows Ã— 9612 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  ILMN_1653355  ...  ethinicity_encoded  gender_encoded\n",
              "0     65      7.650277  ...                  16               0\n",
              "1     66      7.905123  ...                  14               0\n",
              "2     67      7.610028  ...                  16               0\n",
              "3     67      7.647674  ...                  15               0\n",
              "4     67      7.676041  ...                  16               0\n",
              "..   ...           ...  ...                 ...             ...\n",
              "712   83      6.025457  ...                  16               0\n",
              "713   83      6.091774  ...                  16               0\n",
              "714   82      6.088321  ...                  16               1\n",
              "715   88      6.005137  ...                  16               1\n",
              "716   86      6.116739  ...                   7               0\n",
              "\n",
              "[717 rows x 9612 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_XwJtyurcva"
      },
      "source": [
        "X = df.drop('status_encoded',axis=1)\n",
        "y = df['status_encoded']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1KFtwpxrgGb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdsl59yjpZIE"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "lda = LDA()\n",
        "X_train_ = lda.fit_transform(X_train,y_train)\n",
        "X_test_  = lda.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8VNbhfktf1w",
        "outputId": "e260fe92-8f80-4ec6-eade-fa80e792e0f7"
      },
      "source": [
        "X_train_.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(573, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_M_Quj7t1_l",
        "outputId": "83c1314e-d8cc-4092-cd6e-80605c239917"
      },
      "source": [
        "X_test_.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(144, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThMrqYpSpe3b",
        "outputId": "b68ee7bb-0bf7-49d9-957c-826dfb9e4a5c"
      },
      "source": [
        "lda.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.64594606, 0.35405394])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik4TvdhKrjcU"
      },
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# sc = StandardScaler()\n",
        "# X_train_ = sc.fit_transform(X_train)\n",
        "# X_test_  = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXivnhF7ovYS",
        "outputId": "06baea80-40f9-4e63-dd56-8c56ba43f79c"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "707    0\n",
              "283    1\n",
              "206    0\n",
              "502    0\n",
              "144    0\n",
              "      ..\n",
              "109    0\n",
              "331    2\n",
              "6      2\n",
              "536    0\n",
              "577    2\n",
              "Name: status_encoded, Length: 573, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcJDx9XxrlsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa2327e-a176-4f3d-93df-8b4f7ea6bfa1"
      },
      "source": [
        "X_train_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.63983349e+00, -1.45893150e+00],\n",
              "       [ 2.16052673e+00, -1.47963782e+00],\n",
              "       [-3.10050959e+00, -9.40020330e-01],\n",
              "       ...,\n",
              "       [ 9.45429737e-02,  2.75955069e+00],\n",
              "       [-1.16137054e+00, -1.20872837e+00],\n",
              "       [ 5.79054930e-01, -2.24457492e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhJlb6IQlmGJ"
      },
      "source": [
        "configurations_v1 = {\n",
        "    'config1': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    # 'config2': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config3': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    # 'config4': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    # 'config5': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    'config6': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    # 'config7': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config8': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    'config9': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    # 'config10': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config11': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    # 'config12': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': True},\n",
        "    'config13': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    # 'config14': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config15': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    # 'config16': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C':10.0, 'dual':True}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbGGmGR9lpfW"
      },
      "source": [
        "configurations = {\n",
        "    'config1': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    'config2': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config3': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    'config4': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    'config5': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    'config6': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    'config7': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config8': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    'config9': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    'config10': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config11': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    'config12': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': True},\n",
        "    'config13': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    'config14': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config15': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    'config16': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C':10.0, 'dual':True}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGbeqwnSlsQY"
      },
      "source": [
        "configurations_CV = {\n",
        "    'config1': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':10},\n",
        "    'config2': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':20},\n",
        "    'config3': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':10},\n",
        "    'config4': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':20},\n",
        "    'config5': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':20},\n",
        "    'config6': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':10},\n",
        "    'config7': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':20},\n",
        "    'config8': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':10},\n",
        "    'config9': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':10},\n",
        "    'config10': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':20},\n",
        "    'config11': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv':10},\n",
        "    'config12': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv':20},\n",
        "    'config13': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':10},\n",
        "    'config14': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':20},\n",
        "    'config15': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv':10},\n",
        "    'config16': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C':10.0, 'dual':True, 'cv':20}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8CG7lGkluhH"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
        "from math import sqrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6P5SY9Op1fD",
        "outputId": "88669258-e926-428f-8742-0e8187e473e4"
      },
      "source": [
        "X_test_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.25473373e+00, -5.34094612e+00],\n",
              "       [-1.46344197e+00,  1.43367203e+00],\n",
              "       [-1.78999988e+00, -3.37129170e+00],\n",
              "       [ 3.91382319e-01,  1.00674668e+00],\n",
              "       [ 1.40133585e+00,  1.62794470e-02],\n",
              "       [-2.20887307e+00,  1.10639472e+00],\n",
              "       [ 8.00823812e-02,  1.10139380e+00],\n",
              "       [ 4.02698207e+00,  5.78717828e-01],\n",
              "       [-2.78999509e+00,  7.40738639e-01],\n",
              "       [ 3.64877800e+00,  9.80256701e-01],\n",
              "       [-7.16201383e-01, -1.41488354e+00],\n",
              "       [-1.83619547e+00,  5.92681706e-01],\n",
              "       [-2.37234044e+00,  2.36764838e+00],\n",
              "       [-2.98828094e+00, -3.87835029e-01],\n",
              "       [-3.17973507e+00, -6.04417797e-01],\n",
              "       [-8.20039932e-01,  7.14693535e-01],\n",
              "       [ 3.95215431e-01, -1.49787325e+00],\n",
              "       [-5.44527724e+00,  3.12087347e+00],\n",
              "       [ 1.30466058e-01,  1.83819605e-02],\n",
              "       [-4.15280694e+00, -9.29482036e-01],\n",
              "       [-4.43246834e-01,  5.19682901e-01],\n",
              "       [-2.87102904e+00, -1.63358390e+00],\n",
              "       [ 2.24803264e-01, -8.52948357e-01],\n",
              "       [ 9.29381680e-01,  1.47724855e+00],\n",
              "       [-3.10721837e-01,  2.37520490e+00],\n",
              "       [-3.95843037e+00,  2.77990495e+00],\n",
              "       [ 1.15408263e+00,  7.07072850e-01],\n",
              "       [ 3.52232336e+00,  8.74703761e-01],\n",
              "       [ 1.47434635e+00,  9.11728911e-01],\n",
              "       [-2.69718217e+00, -3.71130906e-01],\n",
              "       [-2.10477376e-01, -2.19367624e+00],\n",
              "       [-8.50384254e-01,  1.92613075e+00],\n",
              "       [ 2.54968591e-01,  1.18832449e+00],\n",
              "       [ 2.21963413e-01, -1.33880598e+00],\n",
              "       [ 1.84772812e+01,  1.86033478e+01],\n",
              "       [-2.57458370e+00,  6.63113519e-01],\n",
              "       [-1.48716246e+00,  6.85997268e+00],\n",
              "       [-4.14061492e+00,  3.46980543e+00],\n",
              "       [-3.15751547e+00, -8.08888226e-01],\n",
              "       [-3.32411424e+00, -1.52395480e+00],\n",
              "       [ 1.03063480e+00, -3.44438876e-01],\n",
              "       [-1.72810061e+00,  4.52790702e-01],\n",
              "       [-2.54801096e+00, -2.71188807e-01],\n",
              "       [-8.07614520e-01, -1.04802694e+00],\n",
              "       [-2.80654084e+00, -1.29889601e+00],\n",
              "       [ 1.22218035e+00, -7.29255259e-02],\n",
              "       [ 6.08974840e-01, -1.34214263e+00],\n",
              "       [-5.64145087e-01, -7.95568149e-01],\n",
              "       [-8.06588573e-01,  1.13999934e+00],\n",
              "       [-8.46137025e-01,  1.10462674e+00],\n",
              "       [ 1.03882029e+00,  2.90628526e+00],\n",
              "       [ 1.94066939e-01,  1.85266822e-01],\n",
              "       [-1.04408040e+00, -1.12118896e+00],\n",
              "       [ 1.68322742e+00,  1.28784358e+00],\n",
              "       [ 3.18175374e+00, -1.09852175e+00],\n",
              "       [-9.53048006e-01,  2.50126246e+00],\n",
              "       [-6.56156274e-01,  8.62396448e-01],\n",
              "       [-5.90886343e-01, -9.31949857e-01],\n",
              "       [-4.12577371e+00,  9.99039864e-02],\n",
              "       [-3.28114346e+00, -8.76032057e-01],\n",
              "       [ 2.43314561e+00, -8.47815210e-01],\n",
              "       [-1.52753580e+00, -3.46500029e-01],\n",
              "       [-2.54535999e-01, -6.98689766e-01],\n",
              "       [ 4.23274874e-01, -1.64755494e+00],\n",
              "       [-8.51618620e-01,  2.24927279e+00],\n",
              "       [ 6.77216625e+00,  7.11785006e+00],\n",
              "       [-5.60364972e+00,  9.57435025e+00],\n",
              "       [ 2.07843331e+00,  2.34526497e+00],\n",
              "       [-9.85800116e-01,  3.95787860e-01],\n",
              "       [ 1.08222265e-01, -1.98791300e-01],\n",
              "       [ 3.11477627e+00, -1.73628692e+00],\n",
              "       [ 8.95248519e-01,  1.25450666e+00],\n",
              "       [ 4.00729993e-01,  4.00289565e-01],\n",
              "       [-1.57874967e-01,  3.42940331e+00],\n",
              "       [ 1.48851939e+00, -1.25854047e+00],\n",
              "       [ 5.39059498e-01,  1.21085139e+00],\n",
              "       [ 2.30205301e+00,  1.53817576e+00],\n",
              "       [-3.21747706e+00, -1.24903922e+00],\n",
              "       [-7.77908576e-01, -8.62397030e-01],\n",
              "       [ 9.90892771e-01,  1.86228416e+00],\n",
              "       [-1.00455816e+00,  1.32778064e+00],\n",
              "       [-2.35726702e+00,  9.95610106e-01],\n",
              "       [-1.22487722e+00,  2.02081559e-01],\n",
              "       [-1.08996053e+00,  1.30410119e+00],\n",
              "       [-5.93150503e+00,  3.85651081e+00],\n",
              "       [ 8.59287275e-01,  7.86405923e-01],\n",
              "       [-7.54552698e-02,  2.01658665e-01],\n",
              "       [ 1.50225912e+00, -5.13826155e-01],\n",
              "       [ 2.05214764e-01, -1.35659815e+00],\n",
              "       [ 3.85733352e-01, -1.45475583e+00],\n",
              "       [ 5.15071307e-01,  1.12161833e+00],\n",
              "       [-2.64838446e+00,  5.16148080e-01],\n",
              "       [ 7.78586117e-01,  3.21470567e-01],\n",
              "       [ 5.20751250e-01, -2.02518059e+00],\n",
              "       [-4.06989261e+00,  3.11883378e+00],\n",
              "       [-2.90526525e+00,  1.22191891e+00],\n",
              "       [ 8.30054303e-02,  2.12076519e+00],\n",
              "       [-1.43097213e+00,  2.82524017e-01],\n",
              "       [ 1.67989593e+00, -4.64884335e+00],\n",
              "       [ 1.54096466e+00,  1.71720002e-01],\n",
              "       [-3.69526461e+00,  3.01444244e+00],\n",
              "       [-3.94279832e+00, -7.50519048e+00],\n",
              "       [-9.03730106e-01, -2.06656909e+00],\n",
              "       [-1.05652419e+00, -1.48066779e-01],\n",
              "       [-4.29420291e+00,  3.76469455e+00],\n",
              "       [-1.37063087e+00, -2.84438334e+00],\n",
              "       [ 1.37325059e+00,  1.42448653e+00],\n",
              "       [-3.34782051e-01, -2.75243045e-01],\n",
              "       [ 1.57956957e+00, -5.65011468e-02],\n",
              "       [ 1.15700504e+00, -3.65478061e-01],\n",
              "       [-8.73124076e-01,  5.08618865e-01],\n",
              "       [-5.83954242e-01,  2.33728022e+00],\n",
              "       [-3.35216052e-01, -3.57888663e-01],\n",
              "       [ 4.10043933e+00, -3.69423265e-01],\n",
              "       [-2.40413330e+00,  2.81999261e-01],\n",
              "       [-1.29458398e+00,  3.62238580e-01],\n",
              "       [ 2.92068123e+00, -2.99031146e+00],\n",
              "       [-2.95626721e+00, -7.38167053e-01],\n",
              "       [-1.97285788e+00, -4.69477386e-01],\n",
              "       [-2.17308057e+00, -5.07362359e+00],\n",
              "       [ 1.81308374e+00,  1.33160856e+00],\n",
              "       [ 1.12407766e+00,  2.20425825e+00],\n",
              "       [ 9.37213367e-01,  3.51152633e-01],\n",
              "       [ 5.14129242e-03,  2.01717377e-01],\n",
              "       [-3.67045730e-01, -7.63331684e-01],\n",
              "       [-3.72084617e-01,  1.35944546e+00],\n",
              "       [-1.39525720e+00,  1.99381438e+00],\n",
              "       [-1.13006847e+00,  2.97447022e+00],\n",
              "       [ 2.80090292e-01,  1.71850816e+00],\n",
              "       [-1.05139287e+01, -4.32603504e+00],\n",
              "       [ 8.96956197e-01,  9.49960423e-01],\n",
              "       [ 2.08570862e+00,  1.20858455e+00],\n",
              "       [-5.22913269e-01,  2.36011212e+00],\n",
              "       [-1.06879620e+00,  1.75636944e+00],\n",
              "       [ 2.36733077e+00,  2.70188353e-01],\n",
              "       [-2.16606702e+00, -1.02578581e+00],\n",
              "       [-8.53762955e-02, -3.43305915e+00],\n",
              "       [ 1.17075237e+00,  1.86258294e+00],\n",
              "       [ 7.94946872e-01, -4.30495117e-01],\n",
              "       [ 1.37446445e+00, -9.92859408e-01],\n",
              "       [ 1.93803555e-01, -2.29402684e-01],\n",
              "       [-5.40537820e-01, -9.97253287e-02],\n",
              "       [ 1.31766617e+00,  3.28927139e+00],\n",
              "       [ 4.61237175e-01, -1.39807522e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clxoNrZklwsW",
        "outputId": "3744c77a-02c8-4344-e23b-5edfe9cbfdff"
      },
      "source": [
        "for configs in configurations:\n",
        "  print(\"*************************************************************\")\n",
        "  print(configs)\n",
        "  model = LogisticRegression(solver = configurations[configs]['solver'], random_state=configurations[configs]['random_state'], penalty=configurations[configs]['penalty'], C=configurations[configs]['C'])\n",
        "  model.fit(X_train_, y_train)\n",
        "  print(\"Model Score: \",model.score(X_train_,y_train))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test,model.predict(X_test_)))\n",
        "  print(classification_report(y_test, model.predict(X_test_)))\n",
        "  print(\"RMSE Value (Train)\", sqrt(mean_squared_error(y_train,model.predict(X_train_))))\n",
        "  print(\"RMSE Value (Test): \", sqrt((mean_squared_error(y_test, model.predict(X_test_)))))\n",
        "  print(\"R-Squared Value (Test): \", r2_score(y_test, model.predict(X_test_)))\n",
        "  \n",
        "  print(\"*************************************************************\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************************************************************\n",
            "config1\n",
            "Model Score:  0.9109947643979057\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.40286929011959444\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config2\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config3\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config4\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5763888888888888\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.67      0.62      0.64        42\n",
            "           2       0.42      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.58       144\n",
            "   macro avg       0.57      0.57      0.57       144\n",
            "weighted avg       0.59      0.58      0.58       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.044163673845139\n",
            "R-Squared Value (Test):  -0.602040816326531\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config5\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5763888888888888\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.67      0.62      0.64        42\n",
            "           2       0.42      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.58       144\n",
            "   macro avg       0.57      0.57      0.57       144\n",
            "weighted avg       0.59      0.58      0.58       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.044163673845139\n",
            "R-Squared Value (Test):  -0.602040816326531\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config6\n",
            "Model Score:  0.9109947643979057\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.40286929011959444\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config7\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config8\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config9\n",
            "Model Score:  0.9109947643979057\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.40286929011959444\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config10\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5763888888888888\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.67      0.62      0.64        42\n",
            "           2       0.42      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.58       144\n",
            "   macro avg       0.57      0.57      0.57       144\n",
            "weighted avg       0.59      0.58      0.58       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.044163673845139\n",
            "R-Squared Value (Test):  -0.602040816326531\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config11\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config12\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config13\n",
            "Model Score:  0.9109947643979057\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.40286929011959444\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config14\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5763888888888888\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.67      0.62      0.64        42\n",
            "           2       0.42      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.58       144\n",
            "   macro avg       0.57      0.57      0.57       144\n",
            "weighted avg       0.59      0.58      0.58       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.044163673845139\n",
            "R-Squared Value (Test):  -0.602040816326531\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config15\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config16\n",
            "Model Score:  0.9092495636998255\n",
            "Accuracy Score: 0.5694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62        63\n",
            "           1       0.66      0.60      0.62        42\n",
            "           2       0.41      0.49      0.45        39\n",
            "\n",
            "    accuracy                           0.57       144\n",
            "   macro avg       0.57      0.56      0.56       144\n",
            "weighted avg       0.58      0.57      0.57       144\n",
            "\n",
            "RMSE Value (Train) 0.4050294626561705\n",
            "RMSE Value (Test):  1.0474837574980447\n",
            "R-Squared Value (Test):  -0.6122448979591839\n",
            "*************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugkGfZaclyp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4631a394-b4d4-4199-8a53-87d19914eb33"
      },
      "source": [
        "for configs in configurations_v1:\n",
        "  print(\"*************************************************************\")\n",
        "  print(configs)\n",
        "  model = LogisticRegression(solver = configurations_v1[configs]['solver'], random_state=configurations_v1[configs]['random_state'], penalty=configurations_v1[configs]['penalty'], C=configurations_v1[configs]['C'])\n",
        "  model.fit(X_train_, y_train)\n",
        "  print(\"Model Score: \",model.score(X_train_,y_train))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test,model.predict(X_test_)))\n",
        "  print(classification_report(y_test, model.predict(X_test_)))\n",
        "  print(\"RMSE Value (Train)\", sqrt(mean_squared_error(y_train,model.predict(X_train_))))\n",
        "  print(\"RMSE Value (Test): \", sqrt((mean_squared_error(y_test, model.predict(X_test_)))))\n",
        "  print(\"R-Squared Value (Test): \", r2_score(y_test, model.predict(X_test_)))\n",
        "  \n",
        "  print(\"*************************************************************\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************************************************************\n",
            "config1\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config3\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config6\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config8\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config9\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config11\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config13\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config15\n",
            "Model Score:  0.8\n",
            "Accuracy Score: 0.5888501742160279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       116\n",
            "           1       0.61      0.68      0.64        87\n",
            "           2       0.53      0.38      0.44        84\n",
            "\n",
            "    accuracy                           0.59       287\n",
            "   macro avg       0.58      0.58      0.57       287\n",
            "weighted avg       0.58      0.59      0.58       287\n",
            "\n",
            "RMSE Value (Train) 0.6342914010100947\n",
            "RMSE Value (Test):  0.9770894701881775\n",
            "R-Squared Value (Test):  -0.3948843479494819\n",
            "*************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um48pklGmJp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef38665b-89aa-4f68-81aa-edb306cf07a0"
      },
      "source": [
        "for configs in configurations_CV:\n",
        "  print(\"*************************************************************\")\n",
        "  print(configs)\n",
        "  print(configurations_CV[configs])\n",
        "  model = LogisticRegressionCV(cv=configurations_CV[configs]['cv'],solver = configurations_CV[configs]['solver'], random_state=configurations_CV[configs]['random_state'], penalty=configurations_CV[configs]['penalty'])\n",
        "  model.fit(X_train_, y_train)\n",
        "  print(\"Model Score: \",model.score(X_train_,y_train))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test,model.predict(X_test_)))\n",
        "  print(classification_report(y_test, model.predict(X_test_)))\n",
        "  print(\"RMSE Value (Train)\", sqrt(mean_squared_error(y_train,model.predict(X_train_))))\n",
        "  print(\"RMSE Value (Test): \", sqrt((mean_squared_error(y_test, model.predict(X_test_)))))\n",
        "  print(\"R-Squared Value (Test): \", r2_score(y_test, model.predict(X_test_)))\n",
        "  \n",
        "  print(\"*************************************************************\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************************************************************\n",
            "config1\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config2\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config3\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config4\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config5\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config6\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config7\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config8\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config9\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config10\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config11\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config12\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config13\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config14\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config15\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 10}\n",
            "Model Score:  0.786046511627907\n",
            "Accuracy Score: 0.5679442508710801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.72      0.61       116\n",
            "           1       0.64      0.60      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.57      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6505811355653408\n",
            "RMSE Value (Test):  1.0291906769288925\n",
            "R-Squared Value (Test):  -0.5476089115935856\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config16\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7906976744186046\n",
            "Accuracy Score: 0.5714285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.72      0.61       116\n",
            "           1       0.64      0.61      0.62        87\n",
            "           2       0.56      0.33      0.42        84\n",
            "\n",
            "    accuracy                           0.57       287\n",
            "   macro avg       0.58      0.55      0.55       287\n",
            "weighted avg       0.58      0.57      0.56       287\n",
            "\n",
            "RMSE Value (Train) 0.6469966392206306\n",
            "RMSE Value (Test):  1.0223972648864954\n",
            "R-Squared Value (Test):  -0.5272456364410385\n",
            "*************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}