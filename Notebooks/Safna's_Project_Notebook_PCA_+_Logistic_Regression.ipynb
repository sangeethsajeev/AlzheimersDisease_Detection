{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Safna's Project Notebook - PCA + Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW4BrcFoZUXE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fac1J6xLZgqH",
        "outputId": "d47831f4-9e84-4a43-d04e-db60f1a2db09"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8EnYTlwZnKZ"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Safna's Project/Merged_DF/Merged_DF.csv\",index_col='Unnamed: 0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "vEA0ISVOaBhB",
        "outputId": "94001c52-42d3-45ce-caef-4f826928dc42"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>ILMN_1653355</th>\n",
              "      <th>ILMN_1698554</th>\n",
              "      <th>ILMN_2061446</th>\n",
              "      <th>ILMN_1688755</th>\n",
              "      <th>ILMN_1653165</th>\n",
              "      <th>ILMN_1662364</th>\n",
              "      <th>ILMN_1674698</th>\n",
              "      <th>ILMN_1700461</th>\n",
              "      <th>ILMN_1784269</th>\n",
              "      <th>ILMN_2096191</th>\n",
              "      <th>ILMN_1703743</th>\n",
              "      <th>ILMN_1687609</th>\n",
              "      <th>ILMN_1766054</th>\n",
              "      <th>ILMN_1747627</th>\n",
              "      <th>ILMN_1749001</th>\n",
              "      <th>ILMN_1795507</th>\n",
              "      <th>ILMN_1743205</th>\n",
              "      <th>ILMN_1665730</th>\n",
              "      <th>ILMN_1687840</th>\n",
              "      <th>ILMN_2343048</th>\n",
              "      <th>ILMN_1802404</th>\n",
              "      <th>ILMN_1776119</th>\n",
              "      <th>ILMN_1732140</th>\n",
              "      <th>ILMN_1774344</th>\n",
              "      <th>ILMN_1677814</th>\n",
              "      <th>ILMN_2194009</th>\n",
              "      <th>ILMN_1706531</th>\n",
              "      <th>ILMN_1725675</th>\n",
              "      <th>ILMN_1772189</th>\n",
              "      <th>ILMN_2330267</th>\n",
              "      <th>ILMN_1676846</th>\n",
              "      <th>ILMN_2392635</th>\n",
              "      <th>ILMN_1763875</th>\n",
              "      <th>ILMN_2329927</th>\n",
              "      <th>ILMN_1770031</th>\n",
              "      <th>ILMN_1737475</th>\n",
              "      <th>ILMN_2318685</th>\n",
              "      <th>ILMN_2245305</th>\n",
              "      <th>ILMN_1707925</th>\n",
              "      <th>...</th>\n",
              "      <th>ILMN_1734254</th>\n",
              "      <th>ILMN_1738124</th>\n",
              "      <th>ILMN_1691702</th>\n",
              "      <th>ILMN_1715718</th>\n",
              "      <th>ILMN_1791388</th>\n",
              "      <th>ILMN_1789364</th>\n",
              "      <th>ILMN_1794122</th>\n",
              "      <th>ILMN_1688346</th>\n",
              "      <th>ILMN_1749809</th>\n",
              "      <th>ILMN_1728710</th>\n",
              "      <th>ILMN_2151075</th>\n",
              "      <th>ILMN_1727574</th>\n",
              "      <th>ILMN_2190414</th>\n",
              "      <th>ILMN_1802053</th>\n",
              "      <th>ILMN_1726578</th>\n",
              "      <th>ILMN_1745148</th>\n",
              "      <th>ILMN_1812478</th>\n",
              "      <th>ILMN_1750044</th>\n",
              "      <th>ILMN_1788738</th>\n",
              "      <th>ILMN_1747102</th>\n",
              "      <th>ILMN_1662383</th>\n",
              "      <th>ILMN_1703015</th>\n",
              "      <th>ILMN_1809566</th>\n",
              "      <th>ILMN_1726512</th>\n",
              "      <th>ILMN_1812856</th>\n",
              "      <th>ILMN_2150654</th>\n",
              "      <th>ILMN_1777061</th>\n",
              "      <th>ILMN_1796119</th>\n",
              "      <th>ILMN_1712556</th>\n",
              "      <th>ILMN_2374633</th>\n",
              "      <th>ILMN_1743643</th>\n",
              "      <th>ILMN_1656676</th>\n",
              "      <th>ILMN_2371169</th>\n",
              "      <th>ILMN_1701875</th>\n",
              "      <th>ILMN_1786396</th>\n",
              "      <th>ILMN_1653618</th>\n",
              "      <th>ILMN_2137536</th>\n",
              "      <th>status_encoded</th>\n",
              "      <th>ethinicity_encoded</th>\n",
              "      <th>gender_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65</td>\n",
              "      <td>7.650277</td>\n",
              "      <td>8.148446</td>\n",
              "      <td>8.226970</td>\n",
              "      <td>9.139284</td>\n",
              "      <td>8.237717</td>\n",
              "      <td>9.616626</td>\n",
              "      <td>8.031522</td>\n",
              "      <td>8.309996</td>\n",
              "      <td>7.729847</td>\n",
              "      <td>7.843823</td>\n",
              "      <td>9.662915</td>\n",
              "      <td>8.077568</td>\n",
              "      <td>9.428345</td>\n",
              "      <td>7.865845</td>\n",
              "      <td>7.815536</td>\n",
              "      <td>7.460108</td>\n",
              "      <td>9.002055</td>\n",
              "      <td>7.679191</td>\n",
              "      <td>7.978732</td>\n",
              "      <td>7.610226</td>\n",
              "      <td>7.658746</td>\n",
              "      <td>8.017586</td>\n",
              "      <td>7.467655</td>\n",
              "      <td>7.452172</td>\n",
              "      <td>9.175609</td>\n",
              "      <td>7.381022</td>\n",
              "      <td>9.105610</td>\n",
              "      <td>7.467136</td>\n",
              "      <td>8.087267</td>\n",
              "      <td>7.783330</td>\n",
              "      <td>8.044322</td>\n",
              "      <td>8.720449</td>\n",
              "      <td>9.795684</td>\n",
              "      <td>8.728792</td>\n",
              "      <td>7.803804</td>\n",
              "      <td>7.597712</td>\n",
              "      <td>7.774922</td>\n",
              "      <td>8.102663</td>\n",
              "      <td>7.568479</td>\n",
              "      <td>...</td>\n",
              "      <td>7.921712</td>\n",
              "      <td>7.470530</td>\n",
              "      <td>7.722785</td>\n",
              "      <td>7.863020</td>\n",
              "      <td>9.762821</td>\n",
              "      <td>7.740889</td>\n",
              "      <td>7.486492</td>\n",
              "      <td>7.993250</td>\n",
              "      <td>7.470026</td>\n",
              "      <td>7.638297</td>\n",
              "      <td>7.894740</td>\n",
              "      <td>8.556778</td>\n",
              "      <td>7.638511</td>\n",
              "      <td>8.038134</td>\n",
              "      <td>7.492668</td>\n",
              "      <td>10.104193</td>\n",
              "      <td>8.347094</td>\n",
              "      <td>7.641566</td>\n",
              "      <td>7.616271</td>\n",
              "      <td>7.522899</td>\n",
              "      <td>7.571046</td>\n",
              "      <td>8.670722</td>\n",
              "      <td>8.101310</td>\n",
              "      <td>8.178689</td>\n",
              "      <td>8.788201</td>\n",
              "      <td>8.646717</td>\n",
              "      <td>8.393199</td>\n",
              "      <td>7.646406</td>\n",
              "      <td>8.183159</td>\n",
              "      <td>7.561927</td>\n",
              "      <td>8.245221</td>\n",
              "      <td>10.618034</td>\n",
              "      <td>11.500615</td>\n",
              "      <td>11.972311</td>\n",
              "      <td>8.561519</td>\n",
              "      <td>7.609590</td>\n",
              "      <td>7.441231</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66</td>\n",
              "      <td>7.905123</td>\n",
              "      <td>8.315475</td>\n",
              "      <td>8.030268</td>\n",
              "      <td>8.549185</td>\n",
              "      <td>8.169469</td>\n",
              "      <td>9.835654</td>\n",
              "      <td>7.783841</td>\n",
              "      <td>8.079420</td>\n",
              "      <td>7.833218</td>\n",
              "      <td>7.603615</td>\n",
              "      <td>10.043341</td>\n",
              "      <td>8.112037</td>\n",
              "      <td>10.111046</td>\n",
              "      <td>8.298427</td>\n",
              "      <td>7.436811</td>\n",
              "      <td>7.681562</td>\n",
              "      <td>10.325897</td>\n",
              "      <td>7.583687</td>\n",
              "      <td>7.956272</td>\n",
              "      <td>8.060253</td>\n",
              "      <td>7.665396</td>\n",
              "      <td>7.849625</td>\n",
              "      <td>7.492799</td>\n",
              "      <td>7.555516</td>\n",
              "      <td>8.916246</td>\n",
              "      <td>7.347596</td>\n",
              "      <td>9.084694</td>\n",
              "      <td>7.413566</td>\n",
              "      <td>8.866248</td>\n",
              "      <td>7.688971</td>\n",
              "      <td>7.753245</td>\n",
              "      <td>9.138855</td>\n",
              "      <td>9.917787</td>\n",
              "      <td>8.674218</td>\n",
              "      <td>7.843177</td>\n",
              "      <td>7.625132</td>\n",
              "      <td>7.718231</td>\n",
              "      <td>8.419514</td>\n",
              "      <td>7.454720</td>\n",
              "      <td>...</td>\n",
              "      <td>8.021802</td>\n",
              "      <td>7.501189</td>\n",
              "      <td>7.533398</td>\n",
              "      <td>7.804090</td>\n",
              "      <td>9.656795</td>\n",
              "      <td>7.571105</td>\n",
              "      <td>7.691394</td>\n",
              "      <td>7.993739</td>\n",
              "      <td>7.459030</td>\n",
              "      <td>7.708772</td>\n",
              "      <td>7.917870</td>\n",
              "      <td>8.533683</td>\n",
              "      <td>7.699531</td>\n",
              "      <td>8.161383</td>\n",
              "      <td>7.501998</td>\n",
              "      <td>10.998025</td>\n",
              "      <td>8.980737</td>\n",
              "      <td>7.491495</td>\n",
              "      <td>7.608536</td>\n",
              "      <td>7.525090</td>\n",
              "      <td>7.886355</td>\n",
              "      <td>8.299426</td>\n",
              "      <td>8.054065</td>\n",
              "      <td>8.252783</td>\n",
              "      <td>8.788201</td>\n",
              "      <td>9.546520</td>\n",
              "      <td>8.448941</td>\n",
              "      <td>7.618809</td>\n",
              "      <td>7.999389</td>\n",
              "      <td>7.720191</td>\n",
              "      <td>8.424180</td>\n",
              "      <td>11.289126</td>\n",
              "      <td>11.925122</td>\n",
              "      <td>11.936102</td>\n",
              "      <td>8.697649</td>\n",
              "      <td>7.740773</td>\n",
              "      <td>7.591333</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>67</td>\n",
              "      <td>7.610028</td>\n",
              "      <td>7.912023</td>\n",
              "      <td>7.847137</td>\n",
              "      <td>9.734130</td>\n",
              "      <td>8.173694</td>\n",
              "      <td>9.680070</td>\n",
              "      <td>8.103542</td>\n",
              "      <td>8.282269</td>\n",
              "      <td>7.673322</td>\n",
              "      <td>7.888384</td>\n",
              "      <td>9.822380</td>\n",
              "      <td>7.850279</td>\n",
              "      <td>8.550187</td>\n",
              "      <td>7.832568</td>\n",
              "      <td>7.607402</td>\n",
              "      <td>7.553998</td>\n",
              "      <td>8.904953</td>\n",
              "      <td>7.699037</td>\n",
              "      <td>7.884115</td>\n",
              "      <td>7.766951</td>\n",
              "      <td>7.733130</td>\n",
              "      <td>7.912797</td>\n",
              "      <td>7.573484</td>\n",
              "      <td>7.460622</td>\n",
              "      <td>8.256183</td>\n",
              "      <td>7.584703</td>\n",
              "      <td>8.971329</td>\n",
              "      <td>7.402659</td>\n",
              "      <td>8.098414</td>\n",
              "      <td>7.982504</td>\n",
              "      <td>7.857816</td>\n",
              "      <td>9.216399</td>\n",
              "      <td>9.937217</td>\n",
              "      <td>8.001208</td>\n",
              "      <td>7.747969</td>\n",
              "      <td>7.626327</td>\n",
              "      <td>7.915751</td>\n",
              "      <td>8.132416</td>\n",
              "      <td>7.462959</td>\n",
              "      <td>...</td>\n",
              "      <td>7.838845</td>\n",
              "      <td>7.583151</td>\n",
              "      <td>7.669813</td>\n",
              "      <td>7.672358</td>\n",
              "      <td>9.358193</td>\n",
              "      <td>7.831403</td>\n",
              "      <td>7.452582</td>\n",
              "      <td>7.887924</td>\n",
              "      <td>7.496336</td>\n",
              "      <td>7.804778</td>\n",
              "      <td>7.812287</td>\n",
              "      <td>8.885373</td>\n",
              "      <td>7.773634</td>\n",
              "      <td>8.538723</td>\n",
              "      <td>7.599220</td>\n",
              "      <td>10.435788</td>\n",
              "      <td>8.604181</td>\n",
              "      <td>7.518454</td>\n",
              "      <td>7.549281</td>\n",
              "      <td>7.439370</td>\n",
              "      <td>7.871195</td>\n",
              "      <td>8.619465</td>\n",
              "      <td>8.169948</td>\n",
              "      <td>8.169469</td>\n",
              "      <td>8.619189</td>\n",
              "      <td>9.008516</td>\n",
              "      <td>8.163792</td>\n",
              "      <td>7.581486</td>\n",
              "      <td>8.206669</td>\n",
              "      <td>7.787021</td>\n",
              "      <td>8.278769</td>\n",
              "      <td>11.068854</td>\n",
              "      <td>11.538169</td>\n",
              "      <td>11.714840</td>\n",
              "      <td>8.889266</td>\n",
              "      <td>7.830947</td>\n",
              "      <td>7.711558</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>67</td>\n",
              "      <td>7.647674</td>\n",
              "      <td>8.064728</td>\n",
              "      <td>8.172226</td>\n",
              "      <td>9.832317</td>\n",
              "      <td>8.086709</td>\n",
              "      <td>9.965456</td>\n",
              "      <td>8.131037</td>\n",
              "      <td>8.163628</td>\n",
              "      <td>7.691114</td>\n",
              "      <td>7.644093</td>\n",
              "      <td>10.226752</td>\n",
              "      <td>7.907394</td>\n",
              "      <td>8.584056</td>\n",
              "      <td>7.994788</td>\n",
              "      <td>7.606016</td>\n",
              "      <td>7.735995</td>\n",
              "      <td>9.508634</td>\n",
              "      <td>7.595166</td>\n",
              "      <td>7.643311</td>\n",
              "      <td>7.608362</td>\n",
              "      <td>7.665561</td>\n",
              "      <td>8.000468</td>\n",
              "      <td>7.410198</td>\n",
              "      <td>7.636140</td>\n",
              "      <td>9.738726</td>\n",
              "      <td>7.439620</td>\n",
              "      <td>9.003180</td>\n",
              "      <td>7.467165</td>\n",
              "      <td>8.312623</td>\n",
              "      <td>7.726915</td>\n",
              "      <td>7.442829</td>\n",
              "      <td>9.283740</td>\n",
              "      <td>9.940023</td>\n",
              "      <td>8.489772</td>\n",
              "      <td>7.675784</td>\n",
              "      <td>7.854397</td>\n",
              "      <td>7.909566</td>\n",
              "      <td>8.396654</td>\n",
              "      <td>7.429785</td>\n",
              "      <td>...</td>\n",
              "      <td>7.606291</td>\n",
              "      <td>7.578710</td>\n",
              "      <td>7.803660</td>\n",
              "      <td>7.744011</td>\n",
              "      <td>10.091509</td>\n",
              "      <td>7.584471</td>\n",
              "      <td>7.545805</td>\n",
              "      <td>7.825747</td>\n",
              "      <td>7.435316</td>\n",
              "      <td>7.615556</td>\n",
              "      <td>7.807305</td>\n",
              "      <td>9.041828</td>\n",
              "      <td>7.524073</td>\n",
              "      <td>8.287234</td>\n",
              "      <td>7.374598</td>\n",
              "      <td>10.639769</td>\n",
              "      <td>8.487496</td>\n",
              "      <td>7.589695</td>\n",
              "      <td>7.663742</td>\n",
              "      <td>7.532033</td>\n",
              "      <td>7.909078</td>\n",
              "      <td>8.254265</td>\n",
              "      <td>7.923285</td>\n",
              "      <td>8.242266</td>\n",
              "      <td>8.984602</td>\n",
              "      <td>8.893517</td>\n",
              "      <td>7.935821</td>\n",
              "      <td>7.605167</td>\n",
              "      <td>8.217947</td>\n",
              "      <td>7.560732</td>\n",
              "      <td>8.230026</td>\n",
              "      <td>10.527572</td>\n",
              "      <td>11.685271</td>\n",
              "      <td>12.224006</td>\n",
              "      <td>8.665815</td>\n",
              "      <td>7.744111</td>\n",
              "      <td>7.649596</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>67</td>\n",
              "      <td>7.676041</td>\n",
              "      <td>7.977451</td>\n",
              "      <td>8.144916</td>\n",
              "      <td>9.557300</td>\n",
              "      <td>8.384872</td>\n",
              "      <td>9.763432</td>\n",
              "      <td>8.109347</td>\n",
              "      <td>8.181645</td>\n",
              "      <td>7.581818</td>\n",
              "      <td>7.963825</td>\n",
              "      <td>9.948150</td>\n",
              "      <td>8.220269</td>\n",
              "      <td>9.444721</td>\n",
              "      <td>7.765828</td>\n",
              "      <td>7.538710</td>\n",
              "      <td>7.594694</td>\n",
              "      <td>9.667120</td>\n",
              "      <td>7.611544</td>\n",
              "      <td>7.823204</td>\n",
              "      <td>7.719722</td>\n",
              "      <td>7.766422</td>\n",
              "      <td>8.014104</td>\n",
              "      <td>7.490641</td>\n",
              "      <td>7.427067</td>\n",
              "      <td>8.855090</td>\n",
              "      <td>7.474749</td>\n",
              "      <td>9.359667</td>\n",
              "      <td>7.448467</td>\n",
              "      <td>8.131969</td>\n",
              "      <td>7.678227</td>\n",
              "      <td>7.676978</td>\n",
              "      <td>9.245827</td>\n",
              "      <td>9.933470</td>\n",
              "      <td>8.927955</td>\n",
              "      <td>7.588295</td>\n",
              "      <td>7.601251</td>\n",
              "      <td>8.064056</td>\n",
              "      <td>7.943285</td>\n",
              "      <td>7.497216</td>\n",
              "      <td>...</td>\n",
              "      <td>7.852698</td>\n",
              "      <td>7.542326</td>\n",
              "      <td>7.744761</td>\n",
              "      <td>7.661375</td>\n",
              "      <td>9.741335</td>\n",
              "      <td>7.808566</td>\n",
              "      <td>7.615489</td>\n",
              "      <td>7.918249</td>\n",
              "      <td>7.451993</td>\n",
              "      <td>7.493443</td>\n",
              "      <td>7.575234</td>\n",
              "      <td>8.947312</td>\n",
              "      <td>7.464135</td>\n",
              "      <td>8.111891</td>\n",
              "      <td>7.620563</td>\n",
              "      <td>10.513498</td>\n",
              "      <td>8.466743</td>\n",
              "      <td>7.520534</td>\n",
              "      <td>7.764400</td>\n",
              "      <td>7.431883</td>\n",
              "      <td>7.733685</td>\n",
              "      <td>8.199676</td>\n",
              "      <td>8.067626</td>\n",
              "      <td>8.015104</td>\n",
              "      <td>8.934105</td>\n",
              "      <td>8.967204</td>\n",
              "      <td>8.187785</td>\n",
              "      <td>7.449231</td>\n",
              "      <td>8.056766</td>\n",
              "      <td>7.531862</td>\n",
              "      <td>8.215804</td>\n",
              "      <td>10.833655</td>\n",
              "      <td>11.800960</td>\n",
              "      <td>11.989076</td>\n",
              "      <td>8.785981</td>\n",
              "      <td>7.713783</td>\n",
              "      <td>7.580389</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>83</td>\n",
              "      <td>6.025457</td>\n",
              "      <td>6.191944</td>\n",
              "      <td>7.191870</td>\n",
              "      <td>7.947948</td>\n",
              "      <td>6.755539</td>\n",
              "      <td>8.092437</td>\n",
              "      <td>6.468042</td>\n",
              "      <td>6.403759</td>\n",
              "      <td>6.241325</td>\n",
              "      <td>6.564325</td>\n",
              "      <td>8.000307</td>\n",
              "      <td>6.397057</td>\n",
              "      <td>7.222012</td>\n",
              "      <td>6.504706</td>\n",
              "      <td>6.122697</td>\n",
              "      <td>6.071590</td>\n",
              "      <td>7.646480</td>\n",
              "      <td>6.294688</td>\n",
              "      <td>6.649880</td>\n",
              "      <td>6.198136</td>\n",
              "      <td>6.270404</td>\n",
              "      <td>6.313891</td>\n",
              "      <td>6.099317</td>\n",
              "      <td>6.019087</td>\n",
              "      <td>7.656029</td>\n",
              "      <td>6.215946</td>\n",
              "      <td>7.428026</td>\n",
              "      <td>6.109199</td>\n",
              "      <td>6.852781</td>\n",
              "      <td>6.342467</td>\n",
              "      <td>6.391422</td>\n",
              "      <td>7.225209</td>\n",
              "      <td>8.021324</td>\n",
              "      <td>6.914030</td>\n",
              "      <td>6.080622</td>\n",
              "      <td>6.092792</td>\n",
              "      <td>6.322186</td>\n",
              "      <td>6.436023</td>\n",
              "      <td>5.978622</td>\n",
              "      <td>...</td>\n",
              "      <td>6.299097</td>\n",
              "      <td>6.185424</td>\n",
              "      <td>6.168091</td>\n",
              "      <td>6.403127</td>\n",
              "      <td>7.553589</td>\n",
              "      <td>6.286259</td>\n",
              "      <td>6.145382</td>\n",
              "      <td>6.343409</td>\n",
              "      <td>6.166388</td>\n",
              "      <td>6.154444</td>\n",
              "      <td>6.276808</td>\n",
              "      <td>6.592706</td>\n",
              "      <td>6.430796</td>\n",
              "      <td>7.278737</td>\n",
              "      <td>6.062116</td>\n",
              "      <td>8.739865</td>\n",
              "      <td>6.540948</td>\n",
              "      <td>6.395407</td>\n",
              "      <td>6.063348</td>\n",
              "      <td>6.203769</td>\n",
              "      <td>6.542782</td>\n",
              "      <td>7.240344</td>\n",
              "      <td>6.598752</td>\n",
              "      <td>6.381795</td>\n",
              "      <td>7.397203</td>\n",
              "      <td>7.234947</td>\n",
              "      <td>7.084310</td>\n",
              "      <td>6.210119</td>\n",
              "      <td>6.669304</td>\n",
              "      <td>6.181358</td>\n",
              "      <td>6.866880</td>\n",
              "      <td>8.659101</td>\n",
              "      <td>9.703754</td>\n",
              "      <td>10.998052</td>\n",
              "      <td>7.713113</td>\n",
              "      <td>6.203181</td>\n",
              "      <td>6.340159</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>713</th>\n",
              "      <td>83</td>\n",
              "      <td>6.091774</td>\n",
              "      <td>6.123798</td>\n",
              "      <td>7.382681</td>\n",
              "      <td>8.035608</td>\n",
              "      <td>6.920995</td>\n",
              "      <td>7.913669</td>\n",
              "      <td>6.370507</td>\n",
              "      <td>6.303158</td>\n",
              "      <td>6.339691</td>\n",
              "      <td>6.604721</td>\n",
              "      <td>7.816700</td>\n",
              "      <td>6.435386</td>\n",
              "      <td>7.109822</td>\n",
              "      <td>6.577170</td>\n",
              "      <td>6.024241</td>\n",
              "      <td>5.992126</td>\n",
              "      <td>6.830460</td>\n",
              "      <td>6.406476</td>\n",
              "      <td>6.645352</td>\n",
              "      <td>6.244714</td>\n",
              "      <td>6.112203</td>\n",
              "      <td>6.052734</td>\n",
              "      <td>6.148598</td>\n",
              "      <td>6.047861</td>\n",
              "      <td>6.934621</td>\n",
              "      <td>6.226893</td>\n",
              "      <td>6.879802</td>\n",
              "      <td>6.045579</td>\n",
              "      <td>6.437800</td>\n",
              "      <td>6.399150</td>\n",
              "      <td>6.473523</td>\n",
              "      <td>7.221883</td>\n",
              "      <td>7.990077</td>\n",
              "      <td>6.247180</td>\n",
              "      <td>6.379838</td>\n",
              "      <td>6.067529</td>\n",
              "      <td>6.329688</td>\n",
              "      <td>6.396954</td>\n",
              "      <td>5.976998</td>\n",
              "      <td>...</td>\n",
              "      <td>6.369548</td>\n",
              "      <td>6.041083</td>\n",
              "      <td>6.097158</td>\n",
              "      <td>6.297107</td>\n",
              "      <td>7.268392</td>\n",
              "      <td>6.334440</td>\n",
              "      <td>6.220699</td>\n",
              "      <td>6.597344</td>\n",
              "      <td>6.279882</td>\n",
              "      <td>6.327253</td>\n",
              "      <td>6.208397</td>\n",
              "      <td>6.866831</td>\n",
              "      <td>6.488981</td>\n",
              "      <td>7.308424</td>\n",
              "      <td>6.000767</td>\n",
              "      <td>8.481715</td>\n",
              "      <td>7.263523</td>\n",
              "      <td>6.360071</td>\n",
              "      <td>6.282923</td>\n",
              "      <td>6.328546</td>\n",
              "      <td>6.568216</td>\n",
              "      <td>7.406904</td>\n",
              "      <td>6.615226</td>\n",
              "      <td>6.524513</td>\n",
              "      <td>7.193188</td>\n",
              "      <td>6.820713</td>\n",
              "      <td>6.883545</td>\n",
              "      <td>6.298697</td>\n",
              "      <td>6.722318</td>\n",
              "      <td>6.164023</td>\n",
              "      <td>6.411104</td>\n",
              "      <td>8.149314</td>\n",
              "      <td>9.230673</td>\n",
              "      <td>10.294702</td>\n",
              "      <td>7.363716</td>\n",
              "      <td>6.335564</td>\n",
              "      <td>6.372464</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>82</td>\n",
              "      <td>6.088321</td>\n",
              "      <td>6.255484</td>\n",
              "      <td>7.118513</td>\n",
              "      <td>7.578105</td>\n",
              "      <td>6.752410</td>\n",
              "      <td>7.624695</td>\n",
              "      <td>6.458760</td>\n",
              "      <td>6.456058</td>\n",
              "      <td>6.267209</td>\n",
              "      <td>6.557088</td>\n",
              "      <td>7.637326</td>\n",
              "      <td>6.536048</td>\n",
              "      <td>8.327500</td>\n",
              "      <td>6.531331</td>\n",
              "      <td>6.160537</td>\n",
              "      <td>6.036437</td>\n",
              "      <td>7.621533</td>\n",
              "      <td>6.345700</td>\n",
              "      <td>6.503192</td>\n",
              "      <td>6.143915</td>\n",
              "      <td>6.142310</td>\n",
              "      <td>6.143007</td>\n",
              "      <td>6.059892</td>\n",
              "      <td>6.139461</td>\n",
              "      <td>7.113539</td>\n",
              "      <td>6.245877</td>\n",
              "      <td>7.321108</td>\n",
              "      <td>6.072792</td>\n",
              "      <td>6.365518</td>\n",
              "      <td>6.442756</td>\n",
              "      <td>6.380369</td>\n",
              "      <td>6.956418</td>\n",
              "      <td>8.054158</td>\n",
              "      <td>6.464440</td>\n",
              "      <td>6.478612</td>\n",
              "      <td>6.036811</td>\n",
              "      <td>6.118582</td>\n",
              "      <td>6.387609</td>\n",
              "      <td>6.238373</td>\n",
              "      <td>...</td>\n",
              "      <td>6.412300</td>\n",
              "      <td>6.181855</td>\n",
              "      <td>6.101060</td>\n",
              "      <td>6.384941</td>\n",
              "      <td>7.559047</td>\n",
              "      <td>6.215338</td>\n",
              "      <td>6.141551</td>\n",
              "      <td>6.718023</td>\n",
              "      <td>6.215411</td>\n",
              "      <td>6.302167</td>\n",
              "      <td>6.268427</td>\n",
              "      <td>6.305436</td>\n",
              "      <td>6.433343</td>\n",
              "      <td>7.260361</td>\n",
              "      <td>6.041584</td>\n",
              "      <td>8.409387</td>\n",
              "      <td>7.428963</td>\n",
              "      <td>6.294662</td>\n",
              "      <td>6.082149</td>\n",
              "      <td>6.224265</td>\n",
              "      <td>6.725640</td>\n",
              "      <td>7.288356</td>\n",
              "      <td>6.762998</td>\n",
              "      <td>6.662717</td>\n",
              "      <td>7.280857</td>\n",
              "      <td>6.956466</td>\n",
              "      <td>6.974612</td>\n",
              "      <td>6.210041</td>\n",
              "      <td>6.671604</td>\n",
              "      <td>6.235154</td>\n",
              "      <td>6.942359</td>\n",
              "      <td>8.218770</td>\n",
              "      <td>8.981358</td>\n",
              "      <td>10.267423</td>\n",
              "      <td>7.468304</td>\n",
              "      <td>6.333388</td>\n",
              "      <td>6.358689</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>88</td>\n",
              "      <td>6.005137</td>\n",
              "      <td>6.162781</td>\n",
              "      <td>6.884646</td>\n",
              "      <td>7.810472</td>\n",
              "      <td>6.618831</td>\n",
              "      <td>7.967337</td>\n",
              "      <td>6.605923</td>\n",
              "      <td>6.673488</td>\n",
              "      <td>6.243907</td>\n",
              "      <td>6.541603</td>\n",
              "      <td>7.685420</td>\n",
              "      <td>6.371822</td>\n",
              "      <td>7.782668</td>\n",
              "      <td>6.406314</td>\n",
              "      <td>6.114012</td>\n",
              "      <td>6.111761</td>\n",
              "      <td>7.737141</td>\n",
              "      <td>6.319466</td>\n",
              "      <td>6.498007</td>\n",
              "      <td>6.309278</td>\n",
              "      <td>6.211832</td>\n",
              "      <td>6.305510</td>\n",
              "      <td>6.085181</td>\n",
              "      <td>6.098994</td>\n",
              "      <td>7.256250</td>\n",
              "      <td>6.211291</td>\n",
              "      <td>7.241650</td>\n",
              "      <td>6.032668</td>\n",
              "      <td>6.398309</td>\n",
              "      <td>6.423645</td>\n",
              "      <td>6.368639</td>\n",
              "      <td>7.150617</td>\n",
              "      <td>8.054151</td>\n",
              "      <td>6.705869</td>\n",
              "      <td>6.371576</td>\n",
              "      <td>6.079311</td>\n",
              "      <td>6.106477</td>\n",
              "      <td>6.311087</td>\n",
              "      <td>6.273998</td>\n",
              "      <td>...</td>\n",
              "      <td>6.392218</td>\n",
              "      <td>6.091210</td>\n",
              "      <td>6.186592</td>\n",
              "      <td>6.487828</td>\n",
              "      <td>7.684302</td>\n",
              "      <td>6.291061</td>\n",
              "      <td>6.152289</td>\n",
              "      <td>6.624625</td>\n",
              "      <td>6.120987</td>\n",
              "      <td>6.316855</td>\n",
              "      <td>6.267967</td>\n",
              "      <td>6.582374</td>\n",
              "      <td>6.449674</td>\n",
              "      <td>7.469064</td>\n",
              "      <td>6.025309</td>\n",
              "      <td>8.350234</td>\n",
              "      <td>7.431486</td>\n",
              "      <td>6.192741</td>\n",
              "      <td>6.059229</td>\n",
              "      <td>6.114496</td>\n",
              "      <td>6.785388</td>\n",
              "      <td>7.330857</td>\n",
              "      <td>6.756816</td>\n",
              "      <td>6.544636</td>\n",
              "      <td>7.283032</td>\n",
              "      <td>6.987541</td>\n",
              "      <td>6.908084</td>\n",
              "      <td>6.247840</td>\n",
              "      <td>6.701660</td>\n",
              "      <td>6.223137</td>\n",
              "      <td>6.752188</td>\n",
              "      <td>8.487030</td>\n",
              "      <td>9.137028</td>\n",
              "      <td>10.492294</td>\n",
              "      <td>7.599343</td>\n",
              "      <td>6.431762</td>\n",
              "      <td>6.391373</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>86</td>\n",
              "      <td>6.116739</td>\n",
              "      <td>6.151224</td>\n",
              "      <td>7.056757</td>\n",
              "      <td>8.032515</td>\n",
              "      <td>6.668038</td>\n",
              "      <td>7.993801</td>\n",
              "      <td>6.569850</td>\n",
              "      <td>6.614604</td>\n",
              "      <td>6.372780</td>\n",
              "      <td>6.739602</td>\n",
              "      <td>7.513964</td>\n",
              "      <td>6.253959</td>\n",
              "      <td>8.389697</td>\n",
              "      <td>6.438828</td>\n",
              "      <td>6.125999</td>\n",
              "      <td>6.038305</td>\n",
              "      <td>7.549245</td>\n",
              "      <td>6.366505</td>\n",
              "      <td>6.458320</td>\n",
              "      <td>6.198815</td>\n",
              "      <td>6.184645</td>\n",
              "      <td>6.269684</td>\n",
              "      <td>6.053481</td>\n",
              "      <td>6.011146</td>\n",
              "      <td>6.805501</td>\n",
              "      <td>6.205474</td>\n",
              "      <td>7.155149</td>\n",
              "      <td>5.977905</td>\n",
              "      <td>6.176843</td>\n",
              "      <td>6.443089</td>\n",
              "      <td>6.519355</td>\n",
              "      <td>6.919424</td>\n",
              "      <td>8.123694</td>\n",
              "      <td>6.541376</td>\n",
              "      <td>6.571764</td>\n",
              "      <td>6.088611</td>\n",
              "      <td>6.078007</td>\n",
              "      <td>6.344500</td>\n",
              "      <td>6.204748</td>\n",
              "      <td>...</td>\n",
              "      <td>6.637192</td>\n",
              "      <td>6.030174</td>\n",
              "      <td>6.185430</td>\n",
              "      <td>6.287608</td>\n",
              "      <td>7.145479</td>\n",
              "      <td>6.333991</td>\n",
              "      <td>6.020312</td>\n",
              "      <td>6.937033</td>\n",
              "      <td>6.176610</td>\n",
              "      <td>6.525996</td>\n",
              "      <td>6.068787</td>\n",
              "      <td>6.755403</td>\n",
              "      <td>6.625737</td>\n",
              "      <td>7.120677</td>\n",
              "      <td>6.080683</td>\n",
              "      <td>7.993927</td>\n",
              "      <td>7.772551</td>\n",
              "      <td>6.334878</td>\n",
              "      <td>6.056322</td>\n",
              "      <td>6.119992</td>\n",
              "      <td>7.072445</td>\n",
              "      <td>7.443929</td>\n",
              "      <td>6.540162</td>\n",
              "      <td>6.689672</td>\n",
              "      <td>7.123181</td>\n",
              "      <td>6.885056</td>\n",
              "      <td>6.759775</td>\n",
              "      <td>6.201931</td>\n",
              "      <td>6.800933</td>\n",
              "      <td>6.367431</td>\n",
              "      <td>6.847691</td>\n",
              "      <td>8.143475</td>\n",
              "      <td>8.896626</td>\n",
              "      <td>9.970382</td>\n",
              "      <td>7.348540</td>\n",
              "      <td>6.425247</td>\n",
              "      <td>6.467218</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>717 rows Ã— 9612 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  ILMN_1653355  ...  ethinicity_encoded  gender_encoded\n",
              "0     65      7.650277  ...                  16               0\n",
              "1     66      7.905123  ...                  14               0\n",
              "2     67      7.610028  ...                  16               0\n",
              "3     67      7.647674  ...                  15               0\n",
              "4     67      7.676041  ...                  16               0\n",
              "..   ...           ...  ...                 ...             ...\n",
              "712   83      6.025457  ...                  16               0\n",
              "713   83      6.091774  ...                  16               0\n",
              "714   82      6.088321  ...                  16               1\n",
              "715   88      6.005137  ...                  16               1\n",
              "716   86      6.116739  ...                   7               0\n",
              "\n",
              "[717 rows x 9612 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVWAQ6ZeaHqV",
        "outputId": "2f5bc69a-bc22-4c9b-ed38-1722a82f68e6"
      },
      "source": [
        "df.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gender_encoded    0\n",
              "ILMN_1696183      0\n",
              "ILMN_1685415      0\n",
              "ILMN_2127842      0\n",
              "ILMN_2398903      0\n",
              "                 ..\n",
              "ILMN_1812552      0\n",
              "ILMN_1808047      0\n",
              "ILMN_1796710      0\n",
              "ILMN_1692651      0\n",
              "age               0\n",
              "Length: 9612, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dAiam9h0e-B",
        "outputId": "8ad1d40a-5803-4953-8135-eccec466f8ab"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                     int64\n",
              "ILMN_1653355          float64\n",
              "ILMN_1698554          float64\n",
              "ILMN_2061446          float64\n",
              "ILMN_1688755          float64\n",
              "                       ...   \n",
              "ILMN_1653618          float64\n",
              "ILMN_2137536          float64\n",
              "status_encoded          int64\n",
              "ethinicity_encoded      int64\n",
              "gender_encoded          int64\n",
              "Length: 9612, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_XUkpVeaOcc"
      },
      "source": [
        "X = df.drop('status_encoded',axis=1)\n",
        "y = df['status_encoded']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCD0EqMQdUZy"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca       = PCA(n_components=150)\n",
        "X_PCA = pca.fit_transform(X,y)\n",
        "# _X_test_  = pca.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaPKk1iJbDUn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "_X_train_, _X_test_, y_train, y_test = train_test_split(X_PCA,y,test_size=0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0usmzXnbdLG"
      },
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# sc = StandardScaler()\n",
        "# X_train_ = sc.fit_transform(X_train)\n",
        "# X_test_  = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Pi-XHVGbd6mY",
        "outputId": "eab31201-ea35-4b84-bb3c-17385b592f3f"
      },
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Cumulative Explained Variance')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jtst_x0mw8dZ"
      },
      "source": [
        "configurations_v1 = {\n",
        "    'config1': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    # 'config2': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config3': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    # 'config4': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    # 'config5': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    'config6': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    # 'config7': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config8': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    'config9': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    # 'config10': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config11': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    # 'config12': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': True},\n",
        "    'config13': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    # 'config14': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config15': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    # 'config16': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l1', 'C':10.0, 'dual':True}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI4VxIEI7uto"
      },
      "source": [
        "configurations = {\n",
        "    'config1': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    'config2': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config3': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    'config4': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    'config5': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': False},\n",
        "    'config6': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False},\n",
        "    'config7': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C': 10.0, 'dual': False},\n",
        "    'config8': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False},\n",
        "    'config9': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    'config10': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config11': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    'config12': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l1', 'C': 10.0, 'dual': True},\n",
        "    'config13': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True},\n",
        "    'config14': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C': 1.0, 'dual': True},\n",
        "    'config15': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True},\n",
        "    'config16': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l1', 'C':10.0, 'dual':True}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5aL1Oep56-c"
      },
      "source": [
        "configurations_CV = {\n",
        "    'config1': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':10},\n",
        "    'config2': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':20},\n",
        "    'config3': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':10},\n",
        "    'config4': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':20},\n",
        "    'config5': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':20},\n",
        "    'config6': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv':10},\n",
        "    'config7': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':20},\n",
        "    'config8': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv':10},\n",
        "    'config9': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':10},\n",
        "    'config10': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':20},\n",
        "    'config11': {'solver': 'newton-cg', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv':10},\n",
        "    'config12': {'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv':20},\n",
        "    'config13': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':10},\n",
        "    'config14': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv':20},\n",
        "    'config15': {'solver': 'newton-cg', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv':10},\n",
        "    'config16': {'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C':10.0, 'dual':True, 'cv':20}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gDHZNf1eJZR"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
        "from math import sqrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLrMiZYNy2fJ",
        "outputId": "db26a2b5-0193-40c6-ca0c-9cd3ee73f3ed"
      },
      "source": [
        "for configs in configurations:\n",
        "  print(\"*************************************************************\")\n",
        "  print(configs)\n",
        "  model = LogisticRegression(solver = configurations[configs]['solver'], random_state=configurations[configs]['random_state'], penalty=configurations[configs]['penalty'], C=configurations[configs]['C'])\n",
        "  model.fit(_X_train_, y_train)\n",
        "  print(\"Model Score: \",model.score(_X_train_,y_train))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test,model.predict(_X_test_)))\n",
        "  print(classification_report(y_test, model.predict(_X_test_)))\n",
        "  print(\"RMSE Value (Train)\", sqrt(mean_squared_error(y_train,model.predict(_X_train_))))\n",
        "  print(\"RMSE Value (Test): \", sqrt((mean_squared_error(y_test, model.predict(_X_test_)))))\n",
        "  print(\"R-Squared Value (Test): \", r2_score(y_test, model.predict(_X_test_)))\n",
        "  \n",
        "  print(\"*************************************************************\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************************************************************\n",
            "config1\n",
            "Model Score:  0.8744186046511628\n",
            "Accuracy Score: 0.5331010452961672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.65      0.60       108\n",
            "           1       0.64      0.55      0.59       105\n",
            "           2       0.34      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.52      0.51      0.51       287\n",
            "weighted avg       0.54      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.514894434879391\n",
            "RMSE Value (Test):  1.0206918470772144\n",
            "R-Squared Value (Test):  -0.6800383726849135\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config2\n",
            "Model Score:  0.9209302325581395\n",
            "Accuracy Score: 0.5156794425087108\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.62      0.59       108\n",
            "           1       0.59      0.53      0.56       105\n",
            "           2       0.35      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.52       287\n",
            "   macro avg       0.50      0.50      0.50       287\n",
            "weighted avg       0.52      0.52      0.51       287\n",
            "\n",
            "RMSE Value (Train) 0.4120284478776497\n",
            "RMSE Value (Test):  1.0086731892875196\n",
            "R-Squared Value (Test):  -0.6407063706488119\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config3\n",
            "Model Score:  0.913953488372093\n",
            "Accuracy Score: 0.5296167247386759\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.65      0.61       108\n",
            "           1       0.62      0.53      0.57       105\n",
            "           2       0.36      0.35      0.35        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.51      0.51      0.51       287\n",
            "weighted avg       0.53      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.42040954561773647\n",
            "RMSE Value (Test):  1.0017406453556186\n",
            "R-Squared Value (Test):  -0.6182309409138966\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config4\n",
            "Model Score:  0.858139534883721\n",
            "Accuracy Score: 0.5470383275261324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.69      0.63       108\n",
            "           1       0.64      0.53      0.58       105\n",
            "           2       0.38      0.36      0.37        74\n",
            "\n",
            "    accuracy                           0.55       287\n",
            "   macro avg       0.53      0.53      0.53       287\n",
            "weighted avg       0.55      0.55      0.54       287\n",
            "\n",
            "RMSE Value (Train) 0.5498414147691576\n",
            "RMSE Value (Test):  0.9824239382390308\n",
            "R-Squared Value (Test):  -0.5564235091428797\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config5\n",
            "Model Score:  0.858139534883721\n",
            "Accuracy Score: 0.5470383275261324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.69      0.63       108\n",
            "           1       0.64      0.53      0.58       105\n",
            "           2       0.38      0.36      0.37        74\n",
            "\n",
            "    accuracy                           0.55       287\n",
            "   macro avg       0.53      0.53      0.53       287\n",
            "weighted avg       0.55      0.55      0.54       287\n",
            "\n",
            "RMSE Value (Train) 0.5498414147691576\n",
            "RMSE Value (Test):  0.9824239382390308\n",
            "R-Squared Value (Test):  -0.5564235091428797\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config6\n",
            "Model Score:  0.8744186046511628\n",
            "Accuracy Score: 0.5331010452961672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.65      0.60       108\n",
            "           1       0.64      0.55      0.59       105\n",
            "           2       0.34      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.52      0.51      0.51       287\n",
            "weighted avg       0.54      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.514894434879391\n",
            "RMSE Value (Test):  1.0206918470772144\n",
            "R-Squared Value (Test):  -0.6800383726849135\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config7\n",
            "Model Score:  0.9209302325581395\n",
            "Accuracy Score: 0.519163763066202\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.63      0.59       108\n",
            "           1       0.59      0.53      0.56       105\n",
            "           2       0.35      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.52       287\n",
            "   macro avg       0.50      0.50      0.50       287\n",
            "weighted avg       0.52      0.52      0.52       287\n",
            "\n",
            "RMSE Value (Train) 0.4120284478776497\n",
            "RMSE Value (Test):  1.0017406453556186\n",
            "R-Squared Value (Test):  -0.6182309409138966\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config8\n",
            "Model Score:  0.913953488372093\n",
            "Accuracy Score: 0.5296167247386759\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.65      0.61       108\n",
            "           1       0.62      0.53      0.57       105\n",
            "           2       0.36      0.35      0.35        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.51      0.51      0.51       287\n",
            "weighted avg       0.53      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.42040954561773647\n",
            "RMSE Value (Test):  1.0017406453556186\n",
            "R-Squared Value (Test):  -0.6182309409138966\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config9\n",
            "Model Score:  0.8744186046511628\n",
            "Accuracy Score: 0.5331010452961672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.65      0.60       108\n",
            "           1       0.64      0.55      0.59       105\n",
            "           2       0.34      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.52      0.51      0.51       287\n",
            "weighted avg       0.54      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.514894434879391\n",
            "RMSE Value (Test):  1.0206918470772144\n",
            "R-Squared Value (Test):  -0.6800383726849135\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config10\n",
            "Model Score:  0.858139534883721\n",
            "Accuracy Score: 0.5470383275261324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.69      0.63       108\n",
            "           1       0.64      0.53      0.58       105\n",
            "           2       0.38      0.36      0.37        74\n",
            "\n",
            "    accuracy                           0.55       287\n",
            "   macro avg       0.53      0.53      0.53       287\n",
            "weighted avg       0.55      0.55      0.54       287\n",
            "\n",
            "RMSE Value (Train) 0.5498414147691576\n",
            "RMSE Value (Test):  0.9824239382390308\n",
            "R-Squared Value (Test):  -0.5564235091428797\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config11\n",
            "Model Score:  0.913953488372093\n",
            "Accuracy Score: 0.5296167247386759\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.65      0.61       108\n",
            "           1       0.62      0.53      0.57       105\n",
            "           2       0.36      0.35      0.35        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.51      0.51      0.51       287\n",
            "weighted avg       0.53      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.42040954561773647\n",
            "RMSE Value (Test):  1.0017406453556186\n",
            "R-Squared Value (Test):  -0.6182309409138966\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config12\n",
            "Model Score:  0.9209302325581395\n",
            "Accuracy Score: 0.5156794425087108\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.62      0.59       108\n",
            "           1       0.59      0.53      0.56       105\n",
            "           2       0.35      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.52       287\n",
            "   macro avg       0.50      0.50      0.50       287\n",
            "weighted avg       0.52      0.52      0.51       287\n",
            "\n",
            "RMSE Value (Train) 0.4120284478776497\n",
            "RMSE Value (Test):  1.0086731892875196\n",
            "R-Squared Value (Test):  -0.6407063706488119\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config13\n",
            "Model Score:  0.8744186046511628\n",
            "Accuracy Score: 0.5331010452961672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.65      0.60       108\n",
            "           1       0.64      0.55      0.59       105\n",
            "           2       0.34      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.52      0.51      0.51       287\n",
            "weighted avg       0.54      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.514894434879391\n",
            "RMSE Value (Test):  1.0206918470772144\n",
            "R-Squared Value (Test):  -0.6800383726849135\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config14\n",
            "Model Score:  0.858139534883721\n",
            "Accuracy Score: 0.5470383275261324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.69      0.63       108\n",
            "           1       0.64      0.53      0.58       105\n",
            "           2       0.38      0.36      0.37        74\n",
            "\n",
            "    accuracy                           0.55       287\n",
            "   macro avg       0.53      0.53      0.53       287\n",
            "weighted avg       0.55      0.55      0.54       287\n",
            "\n",
            "RMSE Value (Train) 0.5498414147691576\n",
            "RMSE Value (Test):  0.9824239382390308\n",
            "R-Squared Value (Test):  -0.5564235091428797\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config15\n",
            "Model Score:  0.913953488372093\n",
            "Accuracy Score: 0.5296167247386759\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.65      0.61       108\n",
            "           1       0.62      0.53      0.57       105\n",
            "           2       0.36      0.35      0.35        74\n",
            "\n",
            "    accuracy                           0.53       287\n",
            "   macro avg       0.51      0.51      0.51       287\n",
            "weighted avg       0.53      0.53      0.53       287\n",
            "\n",
            "RMSE Value (Train) 0.42040954561773647\n",
            "RMSE Value (Test):  1.0017406453556186\n",
            "R-Squared Value (Test):  -0.6182309409138966\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config16\n",
            "Model Score:  0.9209302325581395\n",
            "Accuracy Score: 0.519163763066202\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.63      0.59       108\n",
            "           1       0.59      0.53      0.56       105\n",
            "           2       0.35      0.34      0.34        74\n",
            "\n",
            "    accuracy                           0.52       287\n",
            "   macro avg       0.50      0.50      0.50       287\n",
            "weighted avg       0.52      0.52      0.52       287\n",
            "\n",
            "RMSE Value (Train) 0.4120284478776497\n",
            "RMSE Value (Test):  1.0017406453556186\n",
            "R-Squared Value (Test):  -0.6182309409138966\n",
            "*************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnmS4IBW8AWV",
        "outputId": "33c283bf-a63e-4500-a7cd-5c2858220554"
      },
      "source": [
        "for configs in configurations_v1:\n",
        "  print(\"*************************************************************\")\n",
        "  print(configs)\n",
        "  model = LogisticRegression(solver = configurations_v1[configs]['solver'], random_state=configurations_v1[configs]['random_state'], penalty=configurations_v1[configs]['penalty'], C=configurations_v1[configs]['C'])\n",
        "  model.fit(_X_train_, y_train)\n",
        "  print(\"Model Score: \",model.score(_X_train_,y_train))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test,model.predict(_X_test_)))\n",
        "  print(classification_report(y_test, model.predict(_X_test_)))\n",
        "  print(\"RMSE Value (Train)\", sqrt(mean_squared_error(y_train,model.predict(_X_train_))))\n",
        "  print(\"RMSE Value (Test): \", sqrt((mean_squared_error(y_test, model.predict(_X_test_)))))\n",
        "  print(\"R-Squared Value (Test): \", r2_score(y_test, model.predict(_X_test_)))\n",
        "  \n",
        "  print(\"*************************************************************\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************************************************************\n",
            "config1\n",
            "Model Score:  0.8842315369261478\n",
            "Accuracy Score: 0.5555555555555556\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.57      0.71      0.63        65\n",
            "           2       0.48      0.32      0.38        63\n",
            "\n",
            "    accuracy                           0.56       216\n",
            "   macro avg       0.54      0.55      0.54       216\n",
            "weighted avg       0.55      0.56      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.4974987336867905\n",
            "RMSE Value (Test):  1.0206207261596576\n",
            "R-Squared Value (Test):  -0.5191772686067959\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config3\n",
            "Model Score:  0.9121756487025948\n",
            "Accuracy Score: 0.5462962962962963\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.56      0.66      0.61        65\n",
            "           2       0.47      0.33      0.39        63\n",
            "\n",
            "    accuracy                           0.55       216\n",
            "   macro avg       0.53      0.54      0.53       216\n",
            "weighted avg       0.54      0.55      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.42852390193656426\n",
            "RMSE Value (Test):  1.0183501544346312\n",
            "R-Squared Value (Test):  -0.5124253696352101\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config6\n",
            "Model Score:  0.8842315369261478\n",
            "Accuracy Score: 0.5555555555555556\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.57      0.71      0.63        65\n",
            "           2       0.48      0.32      0.38        63\n",
            "\n",
            "    accuracy                           0.56       216\n",
            "   macro avg       0.54      0.55      0.54       216\n",
            "weighted avg       0.55      0.56      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.4974987336867905\n",
            "RMSE Value (Test):  1.0206207261596576\n",
            "R-Squared Value (Test):  -0.5191772686067959\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config8\n",
            "Model Score:  0.9121756487025948\n",
            "Accuracy Score: 0.5462962962962963\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.56      0.66      0.61        65\n",
            "           2       0.47      0.33      0.39        63\n",
            "\n",
            "    accuracy                           0.55       216\n",
            "   macro avg       0.53      0.54      0.53       216\n",
            "weighted avg       0.54      0.55      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.42852390193656426\n",
            "RMSE Value (Test):  1.0183501544346312\n",
            "R-Squared Value (Test):  -0.5124253696352101\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config9\n",
            "Model Score:  0.8842315369261478\n",
            "Accuracy Score: 0.5555555555555556\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.57      0.71      0.63        65\n",
            "           2       0.48      0.32      0.38        63\n",
            "\n",
            "    accuracy                           0.56       216\n",
            "   macro avg       0.54      0.55      0.54       216\n",
            "weighted avg       0.55      0.56      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.4974987336867905\n",
            "RMSE Value (Test):  1.0206207261596576\n",
            "R-Squared Value (Test):  -0.5191772686067959\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config11\n",
            "Model Score:  0.9121756487025948\n",
            "Accuracy Score: 0.5462962962962963\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.56      0.66      0.61        65\n",
            "           2       0.47      0.33      0.39        63\n",
            "\n",
            "    accuracy                           0.55       216\n",
            "   macro avg       0.53      0.54      0.53       216\n",
            "weighted avg       0.54      0.55      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.42852390193656426\n",
            "RMSE Value (Test):  1.0183501544346312\n",
            "R-Squared Value (Test):  -0.5124253696352101\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config13\n",
            "Model Score:  0.8842315369261478\n",
            "Accuracy Score: 0.5555555555555556\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.57      0.71      0.63        65\n",
            "           2       0.48      0.32      0.38        63\n",
            "\n",
            "    accuracy                           0.56       216\n",
            "   macro avg       0.54      0.55      0.54       216\n",
            "weighted avg       0.55      0.56      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.4974987336867905\n",
            "RMSE Value (Test):  1.0206207261596576\n",
            "R-Squared Value (Test):  -0.5191772686067959\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config15\n",
            "Model Score:  0.9121756487025948\n",
            "Accuracy Score: 0.5462962962962963\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59        88\n",
            "           1       0.56      0.66      0.61        65\n",
            "           2       0.47      0.33      0.39        63\n",
            "\n",
            "    accuracy                           0.55       216\n",
            "   macro avg       0.53      0.54      0.53       216\n",
            "weighted avg       0.54      0.55      0.54       216\n",
            "\n",
            "RMSE Value (Train) 0.42852390193656426\n",
            "RMSE Value (Test):  1.0183501544346312\n",
            "R-Squared Value (Test):  -0.5124253696352101\n",
            "*************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIYgBH8V6GIT",
        "outputId": "e14351bb-7db7-463e-97f0-5b7e63146870"
      },
      "source": [
        "for configs in configurations_CV:\n",
        "  print(\"*************************************************************\")\n",
        "  print(configs)\n",
        "  print(configurations_CV[configs])\n",
        "  model = LogisticRegressionCV(cv=configurations_CV[configs]['cv'],solver = configurations_CV[configs]['solver'], random_state=configurations_CV[configs]['random_state'], penalty=configurations_CV[configs]['penalty'])\n",
        "  model.fit(_X_train_, y_train)\n",
        "  print(\"Model Score: \",model.score(_X_train_,y_train))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test,model.predict(_X_test_)))\n",
        "  print(classification_report(y_test, model.predict(_X_test_)))\n",
        "  print(\"RMSE Value (Train)\", sqrt(mean_squared_error(y_train,model.predict(_X_train_))))\n",
        "  print(\"RMSE Value (Test): \", sqrt((mean_squared_error(y_test, model.predict(_X_test_)))))\n",
        "  print(\"R-Squared Value (Test): \", r2_score(y_test, model.predict(_X_test_)))\n",
        "  \n",
        "  print(\"*************************************************************\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************************************************************\n",
            "config1\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config2\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config3\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config4\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config5\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config6\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': False, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config7\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config8\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': False, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config9\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config10\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config11\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config12\n",
            "{'solver': 'liblinear', 'random_state': 0, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config13\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config14\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 1.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config15\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n",
            "*************************************************************\n",
            "config16\n",
            "{'solver': 'liblinear', 'random_state': 1, 'penalty': 'l2', 'C': 10.0, 'dual': True, 'cv': 20}\n",
            "Model Score:  0.7784431137724551\n",
            "Accuracy Score: 0.5740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.74      0.64        88\n",
            "           1       0.64      0.66      0.65        65\n",
            "           2       0.48      0.25      0.33        63\n",
            "\n",
            "    accuracy                           0.57       216\n",
            "   macro avg       0.56      0.55      0.54       216\n",
            "weighted avg       0.56      0.57      0.55       216\n",
            "\n",
            "RMSE Value (Train) 0.6921285402059827\n",
            "RMSE Value (Test):  1.0318986456114838\n",
            "R-Squared Value (Test):  -0.5529367634647246\n",
            "*************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS_NtDc0oXOU"
      },
      "source": [
        "After running different configurations, the following conclusion has been made.\n",
        "\n",
        "\n",
        "Best Logistic Regression Model Configuration found was,\n",
        "Penalty = L2\n",
        "C = 1\n",
        "random_state = 0\n",
        "\n",
        "PCA with number of components 150\n",
        "\n",
        "Model Score = .8423153692614771\n",
        "\n",
        "Since it's a medical dataset, the Recall of the Model plays an integral role in deciding the False Negative Cases. For the Given Dataset, a Recall Value of 71% was achieved.\n"
      ]
    }
  ]
}